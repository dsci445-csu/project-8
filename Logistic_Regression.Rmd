---
title: "Regularization"
output: pdf_document
date: "2024-11-20"
---
## Standardize Data 
```{r}
set.seed(445)

data <- read.csv("heart_failure_clinical_records_dataset.csv")

# scale continuous variables
numeric_vars <- sapply(data, is.numeric)
boolean_vars <- sapply(data, function(x) all(x %in% c(0,1)))

need_scale <- numeric_vars & !boolean_vars

data[need_scale] <- scale(data[need_scale])
```

## Logisitic Regression with No Regularization
```{r}
# convert DEATH_EVENT to a factor 
data <- data %>%
  mutate(DEATH_EVENT = as.factor(DEATH_EVENT))

log_reg_model <- glm(DEATH_EVENT ~ ., data = data, family = binomial)

summary(log_reg_model)
```

It looks like there are quite a few predictors that do not display a significant correlation with the response. Ridge regression seems most appropriate. Will do both ridge regression and lasso and compare 

## Ridge Regression
```{r}
library(tidymodels)
library(tidyverse)
library(ggplot2)

# create vector of 100 lambda values from 0.01 to 10^10
lambda <- lambda <- 10^seq(-2, 10, length.out = 100)
tune_df <- data.frame(lambda = lambda)

# fit ridge regression model for each lambda
prep_data <- recipe(DEATH_EVENT ~ ., data = data)

ridge_ests <- data.frame()

for(lam in lambda) {
  ridge_spec <- logistic_reg(mixture = 0, penalty = lam) |>
    set_mode("classification") |>
    set_engine("glmnet")
  
    workflow() |>
    add_model(ridge_spec) |>
    add_recipe(prep_data) |>
    fit(data) |>
    tidy() |>
    bind_rows(ridge_ests) -> ridge_ests
}

# visualize estimated coefficients for each lambda
ridge_ests |>
  filter(term != "(Intercept)") |>
  ggplot() +
  geom_line(aes(penalty, estimate, group = term, colour = term)) +
  #geom_vline(xintercept = ridge_best$lambda) +
  coord_trans(x = "log10")

# perform 10-fold cross validation, estimate test MSE for each lambda
data_10foldcv <- vfold_cv(data, v = 10)

ridge_spec <- logistic_reg(mixture = 0, penalty = tune("lambda")) |>
  set_mode("classification") |>
  set_engine("glmnet")

workflow() |>
  add_model(ridge_spec) |>
  add_recipe(prep_data) |>
  tune_grid(resamples = data_10foldcv, grid = tune_df, metrics = metric_set(roc_auc, accuracy)) -> ridge_tune

ridge_tune |>
  collect_metrics() |>
  select(lambda, .metric, mean) |>
  pivot_wider(names_from = .metric, values_from = mean) |>
  ggplot() +
  geom_line(aes(lambda, roc_auc)) + 
  geom_point(aes(lambda, roc_auc)) +
  #geom_point(aes(ridge_best$lambda, (ridge_best$mean)^2), color = "red") +
  coord_trans(x = "log10")

# determine best lambda
show_best(ridge_tune, metric = "roc_auc", n = 1)
```


## Lasso
```{r}

# fit lasso model for each lambda
lasso_ests <- data.frame()
for(lam in lambda) {
  lasso_spec <- logistic_reg(mixture = 1, penalty = lam) |>
    set_mode("classification") |>
    set_engine("glmnet")
  
    workflow() |>
    add_model(lasso_spec) |>
    add_recipe(prep_data) |>
    fit(data) |>
    tidy() |>
    bind_rows(lasso_ests) -> lasso_ests
}

lasso_ests |>
  filter(term != "(Intercept)") |>
  ggplot() +
  geom_line(aes(penalty, estimate, group = term, colour = term)) +
  #geom_vline(xintercept = lasso_best$lambda) +
  coord_trans(x = "log10")

lasso_spec <- logistic_reg(mixture = 1, penalty = tune("lambda")) |>
  set_mode("classification") |>
  set_engine("glmnet")

workflow() |>
  add_model(lasso_spec) |>
  add_recipe(prep_data) |>
  tune_grid(resamples = data_10foldcv, grid = tune_df, metrics = metric_set(roc_auc, accuracy)) -> lasso_tune

lasso_tune |>
  collect_metrics() |>
  select(lambda, .metric, mean) |>
  pivot_wider(names_from = .metric, values_from = mean) |>
  ggplot() +
  geom_line(aes(lambda, roc_auc)) +
  geom_point(aes(lambda, roc_auc)) +
  #geom_point(aes(lasso_best$lambda, (lasso_best$mean)^2), color = "red") +
  coord_trans(x = "log10")

show_best(lasso_tune, metric = "roc_auc", n = 1)
```


```{r}
ridge_metrics <- ridge_tune |>
  collect_metrics() |>
  filter(.metric == "roc_auc") |>
  mutate(method = "Ridge")

lasso_metrics <- lasso_tune |>
  collect_metrics() |>
  filter(.metric == "roc_auc") |>
  mutate(method = "Lasso")

combined_metrics <- bind_rows(ridge_metrics, lasso_metrics)

combined_metrics |>
  ggplot(aes(x = lambda, y = mean, color = method)) +
  geom_line() +
  geom_point() +
  #geom_point(aes(ridge_best$lambda, (ridge_best$mean)), color = "green") +
  #geom_point(aes(lasso_best$lambda, (lasso_best$mean), color = "green") +
  coord_trans(x = "log10") +
  labs(
    title = "Comparison of Ridge and Lasso Regression",
    x = "Lambda (Log Scale)",
    y = "roc_auc",
    color = "Method")

ridge_best <- show_best(ridge_tune, metric = "roc_auc", n = 1)
print(ridge_best)

lasso_best <- show_best(lasso_tune, metric = "roc_auc", n = 1)
print(lasso_best)
```


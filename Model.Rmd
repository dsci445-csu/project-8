---
title: "Model"
author: "Paige Galvan"
date: "2024-11-20"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(caTools) 
library(randomForest) 
library(e1071) 
library(ggplot2)
library(dplyr)
library(tidyr)
library(readr)
library(MASS)
library(GGally)
library(tidymodels)
library(ISLR)
library(class)
library(reshape2)
library(pROC)
```

```{r}
set.seed(445)

data <- read.csv("heart_failure_clinical_records_dataset.csv")

sum(is.na(data))

data <- na.omit(data)

data$DEATH_EVENT <- as.factor(data$DEATH_EVENT)

scaled_data <- data
scaled_data[, -which(names(data) == "DEATH_EVENT")] <- scale(data[, -which(names(data) == "DEATH_EVENT")])

summary(scaled_data)
```

#Support Vector Machine

```{r}
set.seed(42)

split <- sample.split(scaled_data$DEATH_EVENT, SplitRatio = 0.8)
train_data <- subset(scaled_data, split == TRUE)
test_data <- subset(scaled_data, split == FALSE)

svm_model <- svm(DEATH_EVENT ~ ., data = train_data, kernel = "radial", cost = 1, scale = TRUE)

summary(svm_model)

predictions <- predict(svm_model, test_data)

conf_matrix <- table(Predicted = predictions, Actual = test_data$DEATH_EVENT)
print(conf_matrix)

accuracy <- sum(predictions == test_data$DEATH_EVENT) / length(predictions)
print(paste("Accuracy: ", round(accuracy, 3)))

```

## Histograms

```{r}

numeric_data <- scaled_data[, sapply(scaled_data, is.numeric)]
melted_data <- melt(numeric_data)

ggplot(melted_data, aes(x = value)) + 
  geom_histogram(bins = 30, fill = "blue", color = "black", alpha = 0.7) +
  facet_wrap(~variable, scales = "free_x") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(title = "Distribution of Numeric Features", x = "Value", y = "Frequency")

```

## Boxplot

```{r}
ggplot(scaled_data, aes(x = DEATH_EVENT, y = ejection_fraction)) + 
  geom_boxplot(fill = c("blue", "red")) +
  theme_minimal() +
  labs(title = "Boxplot of Ejection Fraction by Death Event", x = "Death Event", y = "Ejection Fraction")
```

## SVM Model

```{r}
svm_data <- scaled_data[, c("age", "serum_creatinine", "DEATH_EVENT")]

svm_model_2d <- svm(DEATH_EVENT ~ age + serum_creatinine, data = svm_data, kernel = "radial")

x_range <- seq(min(svm_data$age) - 1, max(svm_data$age) + 1, length.out = 100)
y_range <- seq(min(svm_data$serum_creatinine) - 1, max(svm_data$serum_creatinine) + 1, length.out = 100)
grid <- expand.grid(age = x_range, serum_creatinine = y_range)

predictions_grid <- predict(svm_model_2d, grid)

ggplot(svm_data, aes(x = age, y = serum_creatinine)) +
  geom_point(alpha = 0.7, size = 3) +
  geom_tile(data = grid, 
            aes(x = age, y = serum_creatinine, 
                fill = predictions_grid), 
            alpha = 0.3) +
  scale_fill_manual(values = c("red", "blue")) +
  scale_color_manual(values = c("red", "blue")) +
  theme_minimal() +
  labs(title = "SVM Decision Boundary", x = "Age", y = "Serum Creatinine") +
  theme(legend.position = "none")

```

## Confusion Matrix

```{r}
conf_matrix <- table(Predicted = predictions, Actual = test_data$DEATH_EVENT)

conf_matrix_df <- as.data.frame(conf_matrix)

ggplot(conf_matrix_df, aes(x = Predicted, y = Actual, fill = Freq)) + 
  geom_tile() +
  geom_text(aes(label = Freq), color = "white", size = 8) +
  scale_fill_gradient(low = "white", high = "blue") +
  labs(title = "Confusion Matrix", x = "Predicted", y = "Actual") +
  theme_minimal() +
  theme(axis.text.x = element_text(size = 12), axis.text.y = element_text(size = 12))

```

## ROC Curve

```{r}
svm_probs <- attr(predict(svm_model, test_data, decision.values = TRUE), "decision.values")

roc_curve <- roc(test_data$DEATH_EVENT, svm_probs)
plot(roc_curve, col = "blue", main = "ROC Curve for SVM Model")

```

#Random Forest

```{r}
split <- sample.split(data, SplitRatio = 0.7) 
split

train <- subset(data, split == "TRUE") 
test <- subset(data, split == "FALSE") 

set.seed(120)  
classifier_RF = randomForest(x = train[-5], 
                             y = train$age, 
                             ntree = 500)
classifier_RF

y_pred = predict(classifier_RF, newdata = test[-5])
confusion_mtx = table(test[, 5], y_pred) 
confusion_mtx

plot(classifier_RF)
importance(classifier_RF) 
varImpPlot(classifier_RF)
```

```{r}
set.seed(42)

split <- sample.split(scaled_data$DEATH_EVENT, SplitRatio = 0.8)
train_data <- subset(scaled_data, split == TRUE)
test_data <- subset(scaled_data, split == FALSE)

dim(train_data)
dim(test_data)

rf_model <- randomForest(DEATH_EVENT ~ ., data = train_data, ntree = 500, mtry = 3, importance = TRUE)

importance(rf_model)

tuned_rf <- tuneRF(train_data[, -which(names(train_data) == "DEATH_EVENT")], train_data$DEATH_EVENT, stepFactor = 1.5, improve = 0.01, trace = TRUE)
tuned_rf
```

## Prediction\Confussion Matrix
```{r}
rf_predictions <- predict(rf_model, test_data)

conf_matrix <- table(Predicted = rf_predictions, Actual = test_data$DEATH_EVENT)
print(conf_matrix)

accuracy <- sum(rf_predictions == test_data$DEATH_EVENT) / length(rf_predictions)
cat("Accuracy: ", round(accuracy, 3))
```

##ROC Curve
```{r}

rf_probs <- predict(rf_model, test_data, type = "prob")[, 2] 
roc_curve <- roc(test_data$DEATH_EVENT, rf_probs)

plot(roc_curve, col = "blue", main = "ROC Curve for Random Forest Model")

```

## Variable Importance Plot
```{r}
varImpPlot(rf_model, 
           main = "Variable Importance Plot", 
           type = 1,     
           col = "blue",  
           pch = 16)
```




## I am going to do some cross validation - Neha

```{r}
library(caret)
library(randomForest)
library(e1071)

set.seed(123)
data <- read.csv("heart_failure_clinical_records_dataset.csv")
data$DEATH_EVENT <- as.factor(data$DEATH_EVENT)
scaled_data <- data
scaled_data[, -which(names(data) == "DEATH_EVENT")] <- scale(data[, -which(names(data) == "DEATH_EVENT")])



data$DEATH_EVENT <- factor(data$DEATH_EVENT, levels = c(0, 1), labels = c("No", "Yes"))

scaled_data <- data
scaled_data[, -which(names(data) == "DEATH_EVENT")] <- scale(data[, -which(names(data) == "DEATH_EVENT")])

library(caret)
control <- trainControl(method = "cv", number = 5, classProbs = TRUE, summaryFunction = twoClassSummary)

set.seed(42)
rf_model <- train(
  DEATH_EVENT ~ ., 
  data = scaled_data, 
  method = "rf",
  metric = "ROC",
  trControl = control
)

# SVM with 5-fold Cross-Validation
set.seed(42)
svm_model <- train(
  DEATH_EVENT ~ ., 
  data = scaled_data, 
  method = "svmRadial",
  metric = "ROC",
  trControl = control
)

print(rf_model)
print(svm_model)

rf_results <- rf_model$results
svm_results <- svm_model$results

results <- rbind(
  data.frame(Model = "Random Forest", ROC = rf_model$results$ROC),
  data.frame(Model = "SVM", ROC = svm_model$results$ROC)
)

ggplot(results, aes(x = Model, y = ROC, fill = Model)) +
  geom_bar(stat = "identity") +
  scale_fill_manual(values = c("Random Forest" = "blue", "SVM" = "red")) +
  labs(
    title = "Model ROC Comparison",
    y = "Mean ROC (Cross-Validation)",
    x = "Model"
  ) +
  theme_minimal()

```

## I trained two models, Random Forest and SVM, to classify the data, and compared their performance using 
## cross-validation. The Random Forest model performed best with `mtry = 2`, giving an ROC of 0.91. It was 
## really good at identifying true positives (91% sensitivity) but had a moderate specificity of 69%. The SVM 
## model, tuned with `C = 0.5` and `sigma = 0.0562`, had an ROC of 0.86, a sensitivity of 86%, and a 
## specificity of 66%. Overall, the Random Forest model performed better, especially in terms of overall 
## accuracy and detecting positive cases.


---
title: "DSCI 445 Project Paper"
author: "Paige Galvan, Neha Deshpande, & Witlie Leslie"
date: "2024-11-25"
output: pdf_document
---
```{r, include = FALSE}
library(caTools) 
library(randomForest) 
library(e1071) 
library(ggplot2)
library(dplyr)
library(tidyr)
library(readr)
library(MASS)
library(GGally)
library(tidymodels)
library(ISLR)
library(class)
library(reshape2)
library(pROC)
```


# Motivation

The goal of our project is to predict mortality from heart failure using behavioral risk factor data. Heart 
failure is a disease that affects millions of people yearly. Although modern medicine has improved, it can 
be hard to determine causes of heart failure due to how many variables can affect it. The Heart Failure 
Clinical Records Dataset provides a collection of medical indicators such as age, ejection 
fraction, serum creatinine, and co-existing conditions like diabetes and high blood pressure. By analyzing this
data, researchers can uncover patterns that contribute to better understanding the progression of heart
failure.

The main motivation for our group to study this dataset is to dive a little bit deeper into which factors 
affectheart failure. Knowing that heart failure is a leading cause of death around the world, finding 
meaningful patterns can inform public health strategies, such as targeted lifestyle modifications or health 
carecampaigns. The main objective is to transform this raw data into meaningful conclusions on heart disease. 


# Variables

In our project, we aimed to develop a predictive model to determine the likelihood of death resulting from
heart failure using clinical patient data. The binary response variable, “DEATH_EVENT,” indicates whether
a patient has died (value of 1) or survived (value of 0). The dataset includes 12 risk factor variables, 
comprising of 5 binary variables—anemia status, diabetes status, high blood pressure status, sex, and smoking 
status—and 7 numerical variables, including age, creatine phosphokinase level, ejection fraction, platelet 
concentration, serum creatinine level, serum sodium level, and the length of the follow-up period.

Creatine phosphokinase (CPK) is an enzyme found in the blood that indicates tissue stress, often used to 
assess muscle or heart injury. Ejection fraction refers to the percentage of blood the heart pumps with each 
contraction, crucial for evaluating heart function. Platelet concentration is the number of platelets in a 
unit of blood, important for clotting. Serum creatinine measures creatinine levels in the blood, helping 
assess kidney function, with high levels indicating possible kidney issues. Serum sodium tests the 
concentration of sodium in the blood, vital for nerve and muscle function and fluid balance.

## Exploratory Analysis 
Before applying machine learning models, we began by performing an exploratory analysis of the data. This included assessing the linearity and normality of the predictors, identifying any outliers, and exploring potential correlations among the variables. We visualized distributions using
histograms and box plots to understand the spread of each feature, and scatter plots to check the relationships between the predictor variables and the target variable (mortality). This helped us determine whether the data required transformations before applying machine learning techniques.

Before applying machine learning models, we began by performing an exploratory analysis of the data. This 
included assessing the linearity and normality of the predictors, identifying any outliers, and exploring 
potential correlations among the variables. We visualized distributions using histograms and box plots to 
understand the spread of each feature, and scatter plots to check the relationships between the predictor 
variables and the target variable (mortality). This helped us determine whether the data required 
transformations before applying machine learning techniques.

## Linearity and Normality 

In this analysis, we begin by testing the normality of several continuous variables in the dataset, including 
age, creatinine phosphokinase, ejection fraction, platelets, serum creatinine, and serum sodium. First, we 
used histograms and Q-Q plots to visually inspect the distribution of these variables. The histograms for most
variables indicated skewness, either to the right or left, suggesting that these variables do not follow a 
normal distribution. Platelets and serum sodium appeared to be the most normally distributed, but even they 
showed some deviation from normality. The Q-Q plots confirmed these observations, showing that age, 
ejection fraction, serum sodium, and platelets were closer to a normal distribution, while other variables 
exhibited greater deviations.

To check the normality of the continuous variables, we performed the Shapiro-Wilk test, and all p-values were 
below 0.05, indicating that none of the variables followed a normal distribution.

```{r}
data <- read.csv("heart_failure_clinical_records_dataset.csv")
continuous_vars <- c("age", "creatinine_phosphokinase", "ejection_fraction","platelets", "serum_creatinine", "serum_sodium")
par(mfrow = c(2, 2)) # Set up a grid for multiple plots
for (var in continuous_vars) {
hist(data[[var]], main = paste("Histogram of", var), xlab = var, col = "skyblue", border = "white")
qqnorm(data[[var]], main = paste("Q-Q Plot of", var))
qqline(data[[var]], col = "red")
}
```
Logistic regression analysis showed that predictor variables, such as age and serum creatinine, were significantly associated with the likelihood of a DEATH_EVENT. Residual diagnostics confirmed a good model fit, with minimal issues in linearity or outliers. Testing quadratic terms for the predictors showed no substantial improvement, indicating that the relationships between the variables and DEATH_EVENT were primarily linear.

```{r}
data <- read.csv("heart_failure_clinical_records_dataset.csv")
model <- lm(time ~ age, data = data)
fitted_values <- fitted(model)
residuals <- resid(model)
plot(fitted_values, residuals,
main = "Residuals vs. Fitted Values",
xlab = "Fitted Values",
ylab = "Residuals",
pch = 19, col = "blue")
abline(h = 0, col = "red", lty = 2)
grid()
```


## Polynomial Model

```{r}
data$log_odds <- predict(model, type = "link")  # 'link' gives log-odds

continuous_vars <- c("age", "serum_creatinine", "ejection_fraction", 
                     "creatinine_phosphokinase", "platelets", "serum_sodium", "time")

par(mfrow = c(3, 2)) # Set grid for multiple plots
for (var in continuous_vars) {
  plot(data[[var]], data$log_odds,
       main = paste(var, "vs Log-Odds"),
       xlab = var, ylab = "Log-Odds", col = "blue", pch = 19)
  abline(lm(data$log_odds ~ data[[var]]), col = "red")  # Add a linear trendline
}


poly_model <- glm(DEATH_EVENT ~ poly(age, 2) + poly(serum_creatinine, 2) +
                  poly(ejection_fraction, 2) + poly(creatinine_phosphokinase, 2) +
                  poly(platelets, 2) + poly(serum_sodium, 2) + poly(time, 2) +
                  anaemia + diabetes + high_blood_pressure + sex + smoking,
                  family = binomial, data = data)

summary(poly_model)

```

The logistic regression model includes polynomial terms for several continuous variables, such as age, serum 
creatinine, and ejection fraction, to account for non-linear relationships between these predictors and the 
outcome variable (death event). Using polynomial terms allows the model to capture more complex patterns that 
a simple linear relationship might miss. For example, the effect of age or serum creatinine on the likelihood 
of a death event might not be constant, but could change as these variables increase. The polynomial terms 
help to model this non-linearity, improving the model’s ability to fit the data and providing more accurate 
predictions. This approach ensures that the relationships between predictors and the outcome are represented 
more flexibly, which is particularly important when dealing with real-world data that may not always follow 
linear trends


## Logistic Regression

Logistic regression is a go-to method for binary classification, and we explored three versions to analyze 
predictive performance. First, we fit a basic logistic regression model without regularization as a baseline. 
While simple, it doesn't handle collinearity or irrelevant predictors. Next, we applied ridge regression regularization, 
which penalizes large coefficients to stabilize the model, though it doesn't eliminate predictors, making it 
less interpretable than Lasso. Finally, we used Lasso regularization, which not only penalizes coefficients, but also performs feature selection by shrinking some to zero, improving interpretability. Comparing their predictive power helps determine which approach balances accuracy and simplicity best.

```{r, echo=FALSE, warning=FALSE, message=FALSE}
set.seed(445)

library(tidymodels)
library(tidyverse)
library(ggplot2)
library(dplyr)
library(kableExtra)

data <- read.csv("heart_failure_clinical_records_dataset.csv")

numeric_vars <- sapply(data, is.numeric)
boolean_vars <- sapply(data, function(x) all(x %in% c(0,1)))

need_scale <- numeric_vars & !boolean_vars

data[need_scale] <- scale(data[need_scale])

data <- data %>%
  mutate(DEATH_EVENT = factor(DEATH_EVENT, levels = c("0", "1")))

data_split <- initial_split(data, prop = 0.8)
train_data <- training(data_split)
test_data <- testing(data_split)

prep_data <- recipe(DEATH_EVENT ~ ., data = train_data)
```

Because the predictor variables are of varying ranges and units, we began by scaling all continuous features to prevent our regularization techniques from over-penalizing variables with larger ranges. Next, we split our data into a training set (containing 80% of the data) and a test set (containing 20%) so that we could assess the performance of our logistic regression models using 10-fold cross validation.


```{r, echo=FALSE, warning=FALSE, message=FALSE}
# fit logistic regression model 
logistic_spec_glm <- logistic_reg(penalty = NULL) %>%
  set_mode("classification") %>%
  set_engine("glm")

final_logistic_workflow_glm <- workflow() %>%
  add_model(logistic_spec_glm) %>%
  add_recipe(prep_data)

final_logistic_model_glm <- fit(final_logistic_workflow_glm, data = train_data)

# extract p-values from model
logistic_glm_fit <- final_logistic_model_glm %>% 
  extract_fit_engine() %>% 
  summary()

# create a dataframe of coefficients and p-values
p_values_df <- data.frame(
  Estimate = logistic_glm_fit$coefficients[, "Estimate"],
  `Std. Error` = logistic_glm_fit$coefficients[, "Std. Error"],
  `p value` = logistic_glm_fit$coefficients[, "Pr(>|z|)"]
)

# display p-values
p_values_df %>%
  kbl(caption = "Logistic Regression Coefficients and P-values", digits = 3) %>%
  kable_styling(full_width = F, bootstrap_options = c("striped", "hover", "condensed", "responsive")) %>%
  row_spec(0, bold = T) %>%
  scroll_box(height = "300px") %>%
  kable_styling(latex_options = "hold_position")
```

The logistic regression model found that four predictors displayed significance: age, ejection fraction, serum creatine, and time.

Next, we created a logistic regression model with ridge regression regularization. We determined the optimal lambda penalty value using 10-fold cross validation.

```{r,echo=FALSE, warning=FALSE, message=FALSE}
# create vector of 100 lambda values from 0.01 to 10^10
lambda <- lambda <- 10^seq(-2, 10, length.out = 100)
tune_df <- data.frame(lambda = lambda)

# fit ridge regression model for each lambda
prep_data <- recipe(DEATH_EVENT ~ ., data = data)

ridge_ests <- data.frame()

for(lam in lambda) {
  ridge_spec <- logistic_reg(mixture = 0, penalty = lam) |>
    set_mode("classification") |>
    set_engine("glmnet")
  
    workflow() |>
    add_model(ridge_spec) |>
    add_recipe(prep_data) |>
    fit(data) |>
    tidy() |>
    bind_rows(ridge_ests) -> ridge_ests
}

# visualize estimated coefficients for each lambda
ridge_ests |>
  filter(term != "(Intercept)") |>
  ggplot() +
  labs(title = "Ridge Regression: Coefficient Estimates vs Penalty") +
  geom_line(aes(penalty, estimate, group = term, colour = term)) +
  coord_trans(x = "log10")
```

The graph above depicts the estimated value of each coefficient for each lambda value tested. As the lambda penalty value increases, the coefficient estimates gradually diminish towards zero without being removed entirely.

The graph below depicts the ROC-AUC value for each lambda tested. 10-fold cross validation determined the best penalty to be lambda = 0.0933 with an ROC-AUC value of 0.887, which is indicated on the graph below with a red circle.

```{r, echo=FALSE, warning=FALSE, message=FALSE}
# perform 10-fold cross validation, estimate roc_auc for each lambda
data_10foldcv <- vfold_cv(data, v = 10)

ridge_spec <- logistic_reg(mixture = 0, penalty = tune("lambda")) |>
  set_mode("classification") |>
  set_engine("glmnet")

workflow() |>
  add_model(ridge_spec) |>
  add_recipe(prep_data) |>
  tune_grid(resamples = data_10foldcv, grid = tune_df, metrics = metric_set(roc_auc, accuracy)) -> ridge_tune

# determine best lambda

ridge_metrics <- ridge_tune |>
  collect_metrics() |>
  filter(.metric == "roc_auc") |>
  mutate(method = "Ridge")

ridge_best <- show_best(ridge_tune, metric = "roc_auc", n = 1)

ridge_tune |>
  collect_metrics() |>
  dplyr::select(lambda, .metric, mean) |>
  pivot_wider(names_from = .metric, values_from = mean) |>
  ggplot() +
  labs(title = "Ridge Regression: ROC-AUC vs Lambda") +
  geom_line(aes(lambda, roc_auc)) + 
  geom_point(aes(lambda, roc_auc)) +
  geom_point(aes(ridge_best$lambda, ridge_best$mean), color = "red", shape = 1, size = 4) +
  coord_trans(x = "log10")
```

Once we have chosen the optimal penalty, we can fit the ridge regression model with this lambda value and view its estimates.

```{r, echo=FALSE, warning=FALSE, message=FALSE}

best_ridge_lambda <- show_best(ridge_tune, metric = "roc_auc", n = 1)$lambda

# fit ridge regression model 
ridge_spec_glm <- logistic_reg(penalty = best_ridge_lambda, mixture = 0) %>%
  set_mode("classification") %>%
  set_engine("glm")

final_ridge_workflow_glm <- workflow() %>%
  add_model(ridge_spec_glm) %>%
  add_recipe(prep_data)

final_ridge_model_glm <- fit(final_ridge_workflow_glm, data = train_data)

# extract p-values from model
ridge_glm_fit <- final_ridge_model_glm %>% 
  extract_fit_engine() %>% 
  summary()

# create a dataframe of coefficients and p-values
p_values_df <- data.frame(
  Estimate = ridge_glm_fit$coefficients[, "Estimate"],
  `Std. Error` = ridge_glm_fit$coefficients[, "Std. Error"],
  `p value` = ridge_glm_fit$coefficients[, "Pr(>|z|)"]
)

# display p-values
p_values_df %>%
  kbl(caption = "Ridge Regression Coefficients and P-values", digits = 3) %>%
  kable_styling(full_width = F, bootstrap_options = c("striped", "hover", "condensed", "responsive")) %>%
  row_spec(0, bold = T) %>%
  scroll_box(height = "300px") %>%
  kable_styling(latex_options = "hold_position")
```

The ridge regression model found the same four predictor variables significant as the logistic regression model without regularization: age, ejection fraction, serum creatine, and time. The coefficient estimates and p-values are overall very similar to those of the first logistic regression model. 

For our final logistic regression model, we will implement lasso, once again using 10-fold cross validation to determine the optimal lambda penalty value. 

```{r, echo=FALSE, warning=FALSE, message=FALSE}
# fit lasso model for each lambda
lasso_ests <- data.frame()
for(lam in lambda) {
  lasso_spec <- logistic_reg(mixture = 1, penalty = lam) |>
    set_mode("classification") |>
    set_engine("glmnet")
  
    workflow() |>
    add_model(lasso_spec) |>
    add_recipe(prep_data) |>
    fit(data) |>
    tidy() |>
    bind_rows(lasso_ests) -> lasso_ests
}

# visualize estimated coefficients for each lambda
lasso_ests |>
  filter(term != "(Intercept)") |>
  ggplot() +
  labs(title = "Lasso Method: Coefficient Estimates vs Penalty") +
  geom_line(aes(penalty, estimate, group = term, colour = term)) +
  coord_trans(x = "log10")
```

In the graph above, we can see the coefficient estimates diminish towards zero as lambda increases. The reduction in coefficient estimates is notably steeper for lasso than with ridge regression. Unlike ridge regression, lasso performs feature selection by driving some coefficients to equal zero, thus eliminating them from the model. 

The graph below depicts the ROC-AUC value for each lambda tested. 10-fold cross validation determined the best penalty to be lambda = 0.01 with an ROC-AUC value of 0.879, which is indicated on the graph below with a red circle.

```{r, echo=FALSE, warning=FALSE, message=FALSE}
# perform 10-fold cross validation, estimate roc_auc for each lambda
lasso_spec <- logistic_reg(mixture = 1, penalty = tune("lambda")) |>
  set_mode("classification") |>
  set_engine("glmnet")

workflow() |>
  add_model(lasso_spec) |>
  add_recipe(prep_data) |>
  tune_grid(resamples = data_10foldcv, grid = tune_df, metrics = metric_set(roc_auc, accuracy)) -> lasso_tune

lasso_metrics <- lasso_tune |>
  collect_metrics() |>
  filter(.metric == "roc_auc") |>
  mutate(method = "Lasso")

lasso_best <- show_best(lasso_tune, metric = "roc_auc", n = 1)

# show best lambda with lasso
lasso_tune |>
  collect_metrics() |>
  dplyr::select(lambda, .metric, mean) |>
  pivot_wider(names_from = .metric, values_from = mean) |>
  ggplot() +
  geom_line(aes(lambda, roc_auc)) +
  labs(title = "Lasso Method: ROC-AUC vs Lambda") +
  geom_point(aes(lambda, roc_auc)) +
  geom_point(aes(lasso_best$lambda, lasso_best$mean), color = "red", shape = 1, size = 4) +
  coord_trans(x = "log10")
```

Now with our chosen penalty, we can fit the lasso model with this lambda value and view its estimates.

```{r, echo=FALSE, warning=FALSE, message=FALSE}

best_lasso_lambda <- show_best(lasso_tune, metric = "roc_auc", n = 1)$lambda

# fit lasso model 
lasso_spec_glm <- logistic_reg(penalty = best_lasso_lambda, mixture = 0) %>%
  set_mode("classification") %>%
  set_engine("glm")

final_lasso_workflow_glm <- workflow() %>%
  add_model(lasso_spec_glm) %>%
  add_recipe(prep_data)

final_lasso_model_glm <- fit(final_lasso_workflow_glm, data = train_data)

# extract p-values from model
lasso_glm_fit <- final_lasso_model_glm %>% 
  extract_fit_engine() %>% 
  summary()

# create a dataframe of coefficients and p-values
p_values_df <- data.frame(
  Estimate = lasso_glm_fit$coefficients[, "Estimate"],
  `Std. Error` = lasso_glm_fit$coefficients[, "Std. Error"],
  `p value` = lasso_glm_fit$coefficients[, "Pr(>|z|)"]
)

# display p-values
p_values_df %>%
  kbl(caption = "Lasso Coefficients and P-values", digits = 3) %>%
  kable_styling(full_width = F, bootstrap_options = c("striped", "hover", "condensed", "responsive")) %>%
  row_spec(0, bold = T) %>%
  scroll_box(height = "300px") %>%
  kable_styling(latex_options = "hold_position")
```

The lasso model again found the same four predictor variables significant as both the logistic regression and ridge regression models: age, ejection fraction, serum creatine, and time. The coefficient estimates and p-values are again very similar to those of the logistic regression and ridge regression models.

Because the ridge regression model and the lasso model resulted in estimates very similar to those of the logistic model without regularization, this indicates that regularization was likely unnecessary for our data set. 

Now we will investigate the predictive performance of our three logistic regression models. The metrics we used to assess the performance of these models are ROC-AUC---the area under the receiver-operating characteristic (ROC) curve that represents the probability that the model will correctly rank a randomly selected positive example higher than a negative one---as well as accuracy, which is the proportion of correct predictions out of all total predictions.

Logistic Model Metrics:
```{r, echo=FALSE, warning=FALSE, message=FALSE}
# print logistic model metrics
# fit logistic regression model
logistic_spec <- logistic_reg(penalty = 0) %>%
  set_mode("classification") %>%
  set_engine("glmnet")

final_logistic_workflow <- workflow() %>%
  add_model(logistic_spec) %>%
  add_recipe(prep_data)

final_logistic_model <- fit(final_logistic_workflow, data = train_data)

# evaluate logistic model on test set
logistic_predictions <- predict(final_logistic_model, test_data, type = "prob") %>%
  bind_cols(test_data) %>%
  # creating pred_class column by converting probabilities into class predictions
  mutate(.pred_class = factor(if_else(.pred_1 >= 0.5, 1, 0), levels = levels(DEATH_EVENT)))

logistic_metrics <- roc_auc(logistic_predictions, truth = DEATH_EVENT, .pred_1) %>%
  bind_rows(accuracy(logistic_predictions, truth = DEATH_EVENT, .pred_class))

# print logistic model metrics
print(logistic_metrics)
```

Ridge Model Metrics:
```{r, echo=FALSE, warning=FALSE, message=FALSE}
# finalize ridge with best lambda
best_ridge_lambda <- show_best(ridge_tune, metric = "roc_auc", n = 1)$lambda

final_ridge_spec <- logistic_reg(penalty = best_ridge_lambda, mixture = 0) %>%
  set_mode("classification") %>%
  set_engine("glmnet")

final_ridge_workflow <- workflow() %>%
  add_model(final_ridge_spec) %>%
  add_recipe(prep_data)

final_ridge_model <- fit(final_ridge_workflow, data = train_data)

# evaluate ridge model on test set
ridge_predictions <- predict(final_ridge_model, test_data, type = "prob") %>%
  bind_cols(test_data) %>%
  # creating pred_class column by converting probabilities into class predictions
  mutate(.pred_class = factor(if_else(.pred_1 >= 0.5, 1, 0), levels = levels(DEATH_EVENT)))

ridge_metrics <- roc_auc(ridge_predictions, truth = DEATH_EVENT, .pred_1) %>%
  bind_rows(accuracy(ridge_predictions, truth = DEATH_EVENT, .pred_class))

print(ridge_metrics)
```
Lasso Model Metrics:
```{r, echo=FALSE, warning=FALSE, message=FALSE}
# finalize lasso with best lambda
best_lasso_lambda <- show_best(lasso_tune, metric = "roc_auc", n = 1)$lambda

final_lasso_spec <- logistic_reg(penalty = best_lasso_lambda, mixture = 1) %>%
  set_mode("classification") %>%
  set_engine("glmnet")

final_lasso_workflow <- workflow() %>%
  add_model(final_lasso_spec) %>%
  add_recipe(prep_data)

final_lasso_model <- fit(final_lasso_workflow, data = train_data)

# evaluate lasso model on test set
lasso_predictions <- predict(final_lasso_model, test_data, type = "prob") %>%
  bind_cols(test_data) %>%
  # creating pred_class column by converting probabilities into class predictions
  mutate(.pred_class = factor(if_else(.pred_1 >= 0.5, 1, 0), levels = levels(DEATH_EVENT)))

lasso_metrics <- roc_auc(lasso_predictions, truth = DEATH_EVENT, .pred_1) %>%
  bind_rows(accuracy(lasso_predictions, truth = DEATH_EVENT, .pred_class))

print(lasso_metrics)
```


The 3 logistic models all had very similar performance metrics. This further indicates that regularization was not necessary for this data set. All 3 models had decently high accuracy values (0.83, 0.80, 0.82), but very poor ROC-AUC values (0.145, 0.144, 0.133). Such low ROC-AUC values indicate poor discrimination ability. Because accuracy scores are high and ROC-AUC values are so low, it is likely that accuracy is being erroneously driven up by predictions being assigned to the majority class. The majority class in our data set is DEATH_EVENT = 0, indicating a patient had not deceased during the follow-up period.

Next we will explore alternative modeling methods that might address the limitations and performance issues of our logistic regression models.

## Support Vector Machine


Support Vector Machines (SVMs) are binary classifiers. SVMs can handle multi-class classification including our response variable which is just two classes. One of the key advantages of SVMs is their ability to perform non-linear classification, which increases their flexibility and allows them to handle complex decision boundaries. This handels linear and non-linear decision boundaries. Using a linear kernel is good for approximately linear relationships. We decided to use SVM as our data was only somewhat linear. Earlier we established age, ejection fraction, serum sodium, and platelets were closer to a normal distribution, while other variables had deviations. We also found that all p-values were below 0.05, indicating that none of the variables followed a normal distribution. Using this information we were able to confirm age and serum creatinine were associated with the likelihood of death. Because of this I decided to use age and serum creatinine as our main points of analysis.


We began with our SVM model looking to the Support Vectors and the parameters of interest. The support vector are important to understanding the model's decisions. They are the informative points and make it the most critical for classification. We started by splitting the data into training set, with 80% of the data, and test set, with 20% of our data. We also tuned the SVM model with `C = 0.5` and `sigma = 0.0562`, in order to produce our best results. 



```{r, echo=FALSE, include=FALSE}

library(caTools) 
library(randomForest) 
library(e1071) 
library(ggplot2)
library(dplyr)
library(tidyr)
library(readr)
library(MASS)
library(GGally)
library(tidymodels)
library(ISLR)
library(class)
library(reshape2)
library(pROC)


set.seed(445)

data <- read.csv("heart_failure_clinical_records_dataset.csv")

data <- na.omit(data)

data$DEATH_EVENT <- as.factor(data$DEATH_EVENT)

scaled_data <- data
scaled_data[, -which(names(data) == "DEATH_EVENT")] <- scale(data[, -which(names(data) == "DEATH_EVENT")])


set.seed(42)
scaled_data <- data
library(e1071)             
split <- sample.split(scaled_data$DEATH_EVENT, SplitRatio = 0.8)
train_data <- subset(scaled_data, split == TRUE)
test_data <- subset(scaled_data, split == FALSE)

svm_model <- svm(DEATH_EVENT ~ ., data = train_data, kernel = "radial", cost = 1, scale = TRUE)

predictions <- predict(svm_model, test_data)

conf_matrix <- table(Predicted = predictions, Actual = test_data$DEATH_EVENT)
print(conf_matrix)

accuracy <- sum(predictions == test_data$DEATH_EVENT) / length(predictions)
print(paste("Accuracy: ", round(accuracy, 3)))


```


From this code we conclude that there are only 13 predictive variables we should consider. We also looked into the accuracy of the SVM model this allows us to understand its predictive capabilities. We conclude that this model will have an accuracy of 0.817 this indicates the model has a 81.7% of making correct predictions. Using this information we decided to look into a SVM Descision Boundry. 


```{r, echo=FALSE, warning=FALSE, message=FALSE}
svm_data <- scaled_data[, c("age", "serum_creatinine", "DEATH_EVENT")]

svm_model_2d <- svm(DEATH_EVENT ~ age + serum_creatinine, data = svm_data, kernel = "radial")

x_range <- seq(min(svm_data$age) - 1, max(svm_data$age) + 1, length.out = 100)
y_range <- seq(min(svm_data$serum_creatinine) - 1, max(svm_data$serum_creatinine) + 1, length.out = 100)
grid <- expand.grid(age = x_range, serum_creatinine = y_range)

predictions_grid <- predict(svm_model_2d, grid)

ggplot(svm_data, aes(x = age, y = serum_creatinine)) +
  geom_point(alpha = 0.7, size = 3) +
  geom_tile(data = grid,
            aes(x = age, y = serum_creatinine,
                fill = predictions_grid),
            alpha = 0.3) +
  scale_fill_manual(values = c("red", "blue")) +
  scale_color_manual(values = c("red", "blue")) +
  theme_minimal() +
  labs(title = "SVM Decision Boundary", x = "Age", y = "Serum Creatinine") +
  theme(legend.position = "none")

```


The SVM Boundry analyzes the age vs serum creatinine using the death effect for males vs females. The Red of the grpah represents the Male populations, meanwhile Blue represents the Female population. Overall, serium creatinine and age has little effect on the death in males. Meanwhile age and serum creatinine has a large response on females. 


Finally, to confirm and quantity these results we performed a Receiver Operator Characteristic, a ROC curve is a plot used to show the diagnostic ability of binary classifiers. A ROC curve plots the true positive rate (TPR) against the false positive rate (FPR). In the graph below we analyize the Death Effect and the overall true positive rate of this model.


```{r, echo=FALSE, warning=FALSE, message=FALSE}
svm_probs <- attr(predict(svm_model, test_data, decision.values = TRUE), "decision.values")

roc_curve <- roc(test_data$DEATH_EVENT, svm_probs)
plot(roc_curve, col = "blue", main = "ROC Curve for SVM Model")

```

Using this we can see a ROC Curve of 0.86. This graph demonstrated the Death effect from the SVM Model had a sensitivity of 86%, and a specificity of 66%. Indicating a True Positive rate of 86 and a False Positive Rate of 66.


Using all of the information concluded from this model we can see that it overall performed better than any of the logistic regression approaches. However, I wanted to try one more approach so I began Random Forest. 


## Random Forest

Random Forest is a powerful machine learning algorithm used for both classification and regression. It works by building multiple decision trees and aggregating their predictions to improve accuracy and reduce overfitting. Key advantages include its ability to handle complex, non-linear relationships, manage missing data, and automatically capture feature interactions. Random Forest is also good for overfitting, compared to individual decision trees, and provides a built-in estimate of model performance through error. Additionally, it offers valuable insights into feature importance, helping to identify which variables most influence the outcome. Overall, Random Forest is particularly effective for high-dimensional datasets, imbalanced classes, and when model interpretability is secondary to prediction accuracy.

We decided to use this model as our data was not perfectly linear and had normality issues. We began by splitting the data into training set, with 80% of the data, and test set, with 20% of our data. We found that we should use an Mtry=6 to preform best. 

```{r, echo=FALSE, warning=FALSE, message=FALSE}
set.seed(445)

data <- read.csv("heart_failure_clinical_records_dataset.csv")

sum(is.na(data))

data <- na.omit(data)

data$DEATH_EVENT <- as.factor(data$DEATH_EVENT)

split <- sample.split(data, SplitRatio = 0.7) 


train <- subset(data, split == "TRUE") 
test <- subset(data, split == "FALSE") 

scaled_data <- data
scaled_data[, -which(names(data) == "DEATH_EVENT")] <- scale(data[, -which(names(data) == "DEATH_EVENT")])


set.seed(120)  
classifier_RF = randomForest(x = train[-5], 
                             y = train$age, 
                             ntree = 500)
y_pred = predict(classifier_RF, newdata = test[-5])
confusion_mtx = table(test[, 5], y_pred) 

set.seed(42)

split <- sample.split(scaled_data$DEATH_EVENT, SplitRatio = 0.8)
train_data <- subset(scaled_data, split == TRUE)
test_data <- subset(scaled_data, split == FALSE)

dim(train_data)
dim(test_data)

rf_model <- randomForest(DEATH_EVENT ~ ., data = train_data, ntree = 500, mtry = 3, importance = TRUE)

importance(rf_model)

tuned_rf <- tuneRF(train_data[, -which(names(train_data) == "DEATH_EVENT")], train_data$DEATH_EVENT, stepFactor = 1.5, improve = 0.01, trace = TRUE)
tuned_rf

set.seed(42)

split <- sample.split(scaled_data$DEATH_EVENT, SplitRatio = 0.8)
train_data <- subset(scaled_data, split == TRUE)
test_data <- subset(scaled_data, split == FALSE)


rf_model <- randomForest(DEATH_EVENT ~ ., data = train_data, ntree = 500, mtry = 6, importance = TRUE)

rf_predictions <- predict(rf_model, test_data)

conf_matrix <- table(Predicted = rf_predictions, Actual = test_data$DEATH_EVENT)
print(conf_matrix)

accuracy <- sum(rf_predictions == test_data$DEATH_EVENT) / length(rf_predictions)
cat("Accuracy: ", round(accuracy, 3))
```


Using the information above we found the dimensions of both the training data and testing data. We also broke down the rf_model accuracy of each variable for the model. We analyzed each Mtry and what the OOB error of each would be. Finally we found the confusion matrix and the accuracy of the overall model. The confusion matrix quantifies the model's predictions using this we saw the majority of true labels correctly matched with the predicted labels, indicating high accuracy.  We also found an accuracy score 0.9 from our random forest model. This indicates that our model should offer 90% true positive predictions. Inorder to confirm these results we performed a ROC curve.


```{r, echo=FALSE, warning=FALSE, message=FALSE}

rf_probs <- predict(rf_model, test_data, type = "prob")[, 2]
roc_curve <- roc(test_data$DEATH_EVENT, rf_probs)

plot(roc_curve, col = "blue", main = "ROC Curve for Random Forest Model")

```


With the ROC Curve graph above we found a 0.91 rate. It indicated a 91% sensitivity rate indicating 91% true positive rate. However, this model has a moderate specifity with a 69% rate. 

Overall, the Random Forest model performed better, especially in terms of overall accuracy and detecting positive cases.


# Results

None of the three logistic regression models performed well. The model without regularization performed very similarly to those which implemented ridge regression and lasso, indicating that regularization is unnecessary for this data. ROC-AUC values of 0.10 - 0.13 indicate that the model is performing worse than chance (0.5). A low ROC-AUC value indicates poor discrimination ability. Having a high accuracy value with a low ROC-AUC value might suggest that accuracy is being misleadingly driven up by the model assigning predictions to the majority class (in our case, death event = 0). 

Performing both SVM and Random Forest we were able to conclude our best model was Random Forest. Our SVM held a high accuracy rating and a high ROC-AUC value indicating it would be a good prediction model for our data. However, once we tested the accuracy and ROC-AUC of Random Forest we can see this highly outranks any other options. This model performs the best overall with the predictions to the majority class death event = 0. 


# References

Davide Chicco, Giuseppe Jurman: Machine learning can predict survival of patients with heart failure from serum creatinine and ejection fraction alone. BMC Medical Informatics and Decision Making 20, 16 (2020). https://bmcmedinformdecismak.biomedcentral.com/articles/10.1186/s12911-020-1023-5 

Mount Sinai. "Creatine Phosphokinase Test." Mount Sinai Health Library, https://www.mountsinai.org/health-library/tests/creatine-phosphokinase-test.

Cleveland Clinic. "Ejection Fraction." Cleveland Clinic, https://my.clevelandclinic.org/health/articles/16950-ejection-fraction.

RegenLab. "Platelet Concentration: Concentration Factor." RegenLab, 7 Apr. 2022, https://www.regenlab.com/2022/04/07/platelet-concentration-concentration-factor/#:~:text=Platelet%20concentration%20is%20the%20number,expressed%20in%20millions%20per%20ml.

Kidney Fund. "Serum Creatinine Test." American Kidney Fund, https://www.kidneyfund.org/all-about-kidneys/tests/serum-creatinine-test.

Healthline. "Sodium Blood Test." Healthline, 16 Nov. 2023, https://www.healthline.com/health/sodium-blood#:~:text=A%20sodium%20blood%20test%20is,for%20nerve%20and%20muscle%20function.

 Chicco, Davide, and Giuseppe Jurman. "Machine Learning Can Predict Survival of Patients with Heart Failure from Serum Creatinine and Ejection Fraction Alone." BMC Medical Informatics and Decision Making, vol. 20, no. 16, 2020, https://bmcmedinformdecismak.biomedcentral.com/articles/10.1186/s12911-020-1023-5.



---
title: "Predicting Movie Gross Revenue"
subtitle: "DSCI 445"
author: "Group 8: Kelsey Britton, James Chinnery, Robin Thrush, Kaitlynn Walston"
output: 
  ioslides_presentation:
    css: styles.css
---

```{r setup, include=FALSE, message = FALSE}
set.seed(445)
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)
library(tidymodels)
library(knitr)
library(kableExtra)
library(dplyr)
library(ggplot2)
clean_data <- read_csv("clean_movie_data.csv")
source("linreg.R")
```


## Introduction

Dataset: IMDb movie data (1986–2016), including budget, cast, ratings, genre, and more  
- Goal: Build models that **predict a movie’s gross revenue**  
- Approach:
  - Each group member cleaned specific variables  
  - Combined into one final dataset  
  - Applied multiple modeling methods:
      - Linear Regression  
      - LASSO  
      - Elastic Net  
      - Tree Models 
**Focus: Which predictors influence revenue the most, and which model performs best?**

## Data Cleaning

**Date**

  - Extracted month from "date" column 
  - Used this to assign each entry a season (Fall, Winter, Spring, Summer)
  
**Movie Title**

  - Sentiment analysis using AFINN
  - Added Punctuation
  
**Budget**

  - Almost 1/3 missing or 0
  - Flagged and Filled
  
## Data Cleaning (cont.)

**Director & Company**
  - Calculated frequency

**Country**
  - Assigned to regions
  
**Genre, Rating, & Stars**
  - Also quantified with frequency in dataset

## Pure Linear Regression

- Used cleaned predictors from compiled csv
- Fit on log(gross) scale for stability
- Predictions were converted back to the dollar scale for interpretation

## Model Performance

**$R^2$ = 0.594**

**RMSE = $42.3 million**

**MAE = 19043383**

## 

```{r}
  
top10_table %>%
  kable(format = "html", caption = "Most Significant Predictors") %>%
  kable_styling(full_width = FALSE)
```

##

```{r}

ggplot(plot_df, aes(x = Actual, y = Predicted)) +
  geom_point(alpha = 0.3) +
  geom_abline(intercept = 0, slope = 1, color = "red") +
  scale_x_continuous(labels = scales::dollar) +
  scale_y_continuous(labels = scales::dollar) +
  labs(
    title = "Predicted vs Actual Gross Revenue",
    x = "Actual Gross (USD)",
    y = "Predicted Gross (USD)"
  ) +
  theme_minimal()
```

```{r robin - code I need for the plots to work, include = FALSE}
clean_data <- read_csv("clean_movie_data.csv") 
clean_data <- clean_data |>
  select(-c("X", "country", "director", "name", "writer", 
            "company", "star", "id", "budget_missing", "year", 
            "release_month", "writer_count", "director_count", 
            "direc_freq","star_count"))
clean_data <- clean_data |>
              mutate(rating = ifelse(rating %in% c("UNRATED", "NOT RATED", 
                                                   "Not specified"), 
                                     "Unknown", rating))
clean_data <- clean_data |>
  mutate(gross = log(gross + 1))
net_recipe <- recipe(gross ~ ., data = clean_data) |>
  step_mutate(budget = log(budget+1),
              votes = log(votes +1),
              comp_freq = log(comp_freq + 1),
              runtime = log(runtime + 1),
              score = log(score + 1),
              writer_popularity = log(writer_popularity + 1),
              director_popularity = log(director_popularity +1),
              star_popularity = log(star_popularity + 1)) |>
  step_novel(all_nominal_predictors()) |> ## takes care of factor lvls
  step_unknown(all_nominal_predictors()) |> ## handles missing vals. 
  step_dummy(rating, genre, region, season) |> ## categorical vars.
  step_zv(all_predictors()) |> ## removes zero-variance col.
  step_normalize(all_numeric_predictors()) ## standardizes
elast_net <- linear_reg(penalty = tune(), mixture = tune()) |>
  set_engine("glmnet") 

net_workflow <- workflow() |>
  add_recipe(net_recipe) |>
  add_model(elast_net)
folds <- vfold_cv(clean_data, v = 10)
grid_net <- grid_regular(penalty(),
                         mixture(range = c(0,1)),
                         levels = 20)
the_metrics <- metric_set(rmse, rsq, mae)
tuned_results <- tune_grid(net_workflow,
                           resamples = folds,
                           grid = grid_net,
                           control = control_grid(save_pred = TRUE),
                           metrics = the_metrics)
best_parameters <- tuned_results |>
  select_best(metric = "rmse")
final_net <- finalize_workflow(net_workflow, best_parameters)
final_fit <- fit(final_net, data = clean_data)

preds <- predict(final_fit, clean_data) |>
  bind_cols(clean_data)
glm_fit <- extract_fit_parsnip(final_fit)$fit
lambda_used <- best_parameters$penalty

coefs <- coef(glm_fit, s = lambda_used)
coef_dense <- as.matrix(coefs)
non_z_co <- coef_dense[coef_dense[,1] != 0 & 
                         rownames(coef_dense) != "(Intercept)", , drop = FALSE]
coef_df <- data.frame(predictor = rownames(non_z_co),
                      coefficient = non_z_co[,1])
```

## Elastic Net

- Reminder:
Elastic Net is a shrinkage model that combines LASSO and Ridge Regression
penalties.

- Why Elastic Net:
  - Large amount of predictors that aren't meaningful are dropped
  
  - Handles colinearity well like assuming comp_freq and director popularity
  
  - Able to rank predictors
  - Stabilize coefficients of correlated predictors

Since our primary goal was to identify strongest predictors, we did not remove possible correlated terms and did not create interaction terms. 


## Handling Data: 

- How: 

** Log transformed gross (response variable) 

** Log +1 transformed all numeric predictors

** Dummy encoded the categorical variables: Genre, Region, Rating and Season

10 fold cross-validation 

## Hyperparameter Tuning:

Grid of 400 different combos of alpha and lambda
Each combo used to compute metrics for RMSE, MAE, and R^2

The best RMSE (lowest CV RMSE) was pulled out and revealed the ideal lambda and 
alpha values. 

Finally we are able to pass this back to the workflow and come up with the best 
elastic net model. 

**Cross Validation:**

Best Lambda: 0.02636651

Best Alpha: 0.4736842

RMSE: 1.575973  

$R^2$: 0.584

MAE: 1.13

```{r MAECV}
exp(1.5759 ) -1
exp(1.564) -1
exp(1.13)
```


## Prediction Model: 

**Prediction:**

RMSE: 1.564085

R^2 : 0.5909

MAE: 1.120881

```{r MAEPred}
exp(1.12)
```

The RMSE from cross-validation (1.57) and RMSE from prediction (1.56) are very 
close, indicating the model is stable and not overfitting.

```{r avg prediction err}
1.564085/mean(clean_data$gross)

```

The RMSE relative to the average gross is abt. 10%. AKA the 
model predicts movie revenue with roughly +/- 10% accuracy.

##

```{r actual v pred visual}
act_pred <- ggplot(data = preds, mapping = aes(x = gross, y = .pred)) +
  geom_point(alpha = 0.2) +
  geom_abline(intercept = 0, slope = 1, color = "darkred") + 
  labs(title = "Actual VS Predicted Gross",
       x = "Actual Log Gross",
       y = "Predicted Log Gross") 
act_pred
```

## Predictors:

Total predictors: 53

Leftover: 39 and Shrunk to Zero: 14

**Strongest predictors:**
- Votes (0.966)
- comp_freq (0.444)
- rating_Unknown (-0.441)

**Moderate Predictors:**
- Director_popularity (0.225)
- Runtime (0.179)
- Budget (0.113)


**Predictors that shrunk to 0:**
- Sentiment
- Oceania
- Genre: Musical

##

```{r predictor barplot}

coef_df <- coef_df |>
  mutate(abs_coef = abs(coefficient)) |>
  arrange(desc(abs_coef))

top_coef <- coef_df |>
  filter(rank(-abs_coef) <= 15)

bar_pred <- ggplot(data = top_coef, aes(x = reorder(predictor, abs_coef), 
                                        y = abs_coef, fill = coefficient > 0)) + 
  geom_col() + 
  coord_flip() + 
  scale_fill_manual(values = c("darkred", "darkgreen"), 
                    labels = c("Negative", "Positive")) +
  labs(title = "Predictor Importance",
       x = "Predictor",
       y = "Coefficient Magnitude",
       fill = "Coefficient Sign")
bar_pred
```


## Elastic Net Takeaway:

- Identified important predictors
- Predicts movie gross within ~10% of the actual gross
- High-gross movies are predicted more accurately than low-gross movies
- Demonstrates stability as it performs very similarly between cross-validation
and full-data predictions. 


## LASSO Model: 

```{r lasso, warning=FALSE, message = FALSE}

library(glmnet)
library(dplyr)
library(tidyr)
library(stringr)

# data
ds <- read.csv("clean_movie_data.csv")
df <- na.omit(ds)

top_stars <- names(sort(table(df$star), decreasing=TRUE))[1:50]
df$star <- ifelse(df$star %in% top_stars, df$star, "Other")
df$star <- as.factor(df$star)

# response var
y <- df$gross

# predictors 
predictors <- df %>%
  select(gross, budget, runtime, year, score, rating, genre, star, 
         season, budget_missing, direc_freq, director_count, director_popularity,
         comp_freq, writer_count, writer_popularity, star_count, star_popularity,
         sentiment_best)
predictors <- predictors %>% select(-gross)

# model matrix (dummy coding)
X <- model.matrix(~ ., data=predictors)[, -1]

# cv lasso
cv_lasso <- cv.glmnet(x=X, y=y, alpha=1, nfolds = 10, standardize = TRUE)
lamb_min <- cv_lasso$lambda.min
lamb_1se <- cv_lasso$lambda.1se

# coefs at lasso min
coef_lasso <- coef(cv_lasso, s="lambda.min")
coef_lasso <- coef_lasso[coef_lasso != 0]

pred <- predict(cv_lasso, newx=X, s="lambda.min")

coef_clean <- coef(cv_lasso, s = "lambda.min") %>%
  as.matrix() %>%
  as.data.frame() %>%
  rownames_to_column("term") %>%
  rename(coefficient = 2) %>%
  filter(coefficient != 0) %>%
  mutate(
    variable = case_when(
      term == "(Intercept)" ~ "(Intercept)",
      str_starts(term, "star") ~ "star",
      str_starts(term, "genre") ~ "genre",
      str_starts(term, "rating") ~ "rating",
      TRUE ~ term         # numeric variables
    ),

    # Extract category level (only for factors)
    level = case_when(
      variable %in% c("star", "genre", "rating") ~ 
        str_remove(term, paste0(variable)),
      TRUE ~ NA_character_
    ),
    level = str_trim(level),

    coefficient = round(coefficient, 2),
    abs_coef = round(abs(coefficient), 2),
  ) %>%
  select(variable, level, coefficient, abs_coef) %>%
  arrange(desc(abs_coef))

head(coef_clean)

```

## Results

```{r, echo=FALSE}
pred <- predict(cv_lasso, newx=X, s="lambda.min")
rmse <- sqrt(mean((y-pred)^2))
cat("LASSO rmse: $",rmse)
```

- The baseline/intercept we are given is a gross of ~554 million dollars. 
- The coefficients we see are based on the income compared to the baseline. For example, Tom Hanks would be predicted to earn about 28.4 million dollars more than the assumed baseline, but Nicole Kidman would be predicted to make about 27.2 million dollars less than the baseline.
- This lasso model identified leading actors was the most influential predictor
- RMSE = 38.5 million dollars, meaning that on average, this lasso model's predictions for movie revenue is off by about 38.5 million dollars

## Tree Based Models

```{r, include=F}
rf <- readRDS("final_rf_results.rds")
rf_plot_data <- collect_predictions(rf) %>%
  mutate(
    pred_dollars = exp(.pred),
    truth_dollars = exp(gross)
  )
binnedrf <- readRDS("binned_rf_results.rds")
binned_plot_data <- collect_predictions(binnedrf) %>%
  mutate(
    pred_dollars = exp(.pred),
    truth_dollars = exp(gross)
  )
combined_plot_data <- bind_rows(
  rf_plot_data %>% mutate(Model = "Target Encoding"),
  binned_plot_data %>% mutate(Model = "Binary Threshold Encoding")
)
r2_target <- collect_metrics(rf) %>% 
  filter(.metric == "rsq") %>% 
  pull(.estimate) %>% 
  round(3)

r2_binary <- collect_metrics(binnedrf) %>% 
  filter(.metric == "rsq") %>% 
  pull(.estimate) %>% 
  round(3)
```

```{r compare-plot, echo=FALSE, fig.width=9, fig.height=4.5, out.width="100%"}
plot_labels <- tibble(
  Model = c("Target Encoding", "Binary Threshold Encoding"),  
  label = c(paste0("R2: ", r2_target), paste0("R2: ", r2_binary)),               
  x     = c(1e8, 1e8),                             
  y     = c(1e4, 1e4)                              
)

ggplot(combined_plot_data, aes(x = truth_dollars, y = pred_dollars)) +
  
  geom_point(alpha = 0.3, size = 1.5, color = "#2c3e50") +
  
  geom_abline(lty = 2, color = "#e74c3c", linewidth = 1) +
  
  facet_wrap(~Model) + 
  
  geom_text(
    data = plot_labels,             
    aes(x = x, y = y, label = label), 
    inherit.aes = FALSE,           
    size = 5
  ) +
  
  scale_x_log10(labels = label_dollar(scale_cut = cut_short_scale())) +
  scale_y_log10(labels = label_dollar(scale_cut = cut_short_scale())) +
  
  # Labels
  labs(
    x = "Actual Revenue",
    y = "Predicted Revenue",
    title = "Predicted vs Actual Gross Revenue with Random Forest",
  ) +
  theme_minimal(base_size = 14) +
  theme(panel.spacing = unit(2, "lines")) # Add space between plots
```

## Random Forest Performance {.columns-2}
<br>

```{r calc-metrics, echo=FALSE, fig.width=6, out.width="100%"}
mape_val <- binned_plot_data %>%
  summarise(
    # Formula: Median( Abs( (Actual - Pred) / Actual ) )
    score = median( abs((truth_dollars - pred_dollars) / truth_dollars) )
  ) %>%
  pull(score)

perf_metrics <- tibble(
  Metric = c("R-Squared", "RMSE (Dollars)", "MAE (Dollars)", "MdAPE (Avg % Error)"),
  Value = c(
    r2_binary, 
    scales::dollar(rmse(binned_plot_data, truth = truth_dollars, estimate = pred_dollars)$.estimate),
    scales::dollar(mae(binned_plot_data, truth = truth_dollars, estimate = pred_dollars)$.estimate),
    scales::percent(mape_val, accuracy = 1) # <--- New Line
  )
)


# 2. Calculate RMSE vs Gross Revenue (Plot)
# We split movies into 10 "Revenue Tiers" (Deciles) to see how error changes
rmse_by_gross <- binned_plot_data %>%
  mutate(gross_bin = ntile(truth_dollars, 10)) %>%
  group_by(gross_bin) %>%
  summarise(
    avg_gross = mean(truth_dollars),
    bin_rmse  = rmse_vec(truth = truth_dollars, estimate = pred_dollars)
  )

perf_metrics %>%
  kbl() %>%
  kable_styling(font_size = 20, bootstrap_options = "striped")
```
<br>

* Poor model relative to the median gross revenue ~14m

* Has some utility predicting high vs low performers

## Overall Results and Conclusion

```{r plots, echo=FALSE, out.width="100%", fig.height= 8}

ggplot(rmse_by_gross, aes(x = avg_gross, y = bin_rmse)) +
  geom_line(color = "#2c3e50", linewidth = 1) +
  geom_point(color = "#e74c3c", size = 3) +
  
  scale_x_log10(labels = label_dollar(scale_cut = cut_short_scale())) +
  scale_y_log10(labels = label_dollar(scale_cut = cut_short_scale())) +
  
  labs(
    title = "RMSE vs. Revenue",
    x = "Avg Gross Revenue",
    y = "RMSE (Error)"
  ) +
  theme_minimal(base_size = 14)
```

## Data Limitations 


```{r resid-plot, echo=FALSE, fig.width=10, fig.height=4.5, out.width="100%"}
# 1. Calculate Residuals
resid_data <- binned_plot_data %>%
  mutate(residual = truth_dollars - pred_dollars)

# 2. Plot
ggplot(resid_data, aes(x = pred_dollars, y = residual)) +

  geom_hline(yintercept = 0, color = "#e74c3c", linewidth = 1.5, linetype = "dashed") +
  
  geom_point(alpha = 0.4, size = 2.5, color = "#2c3e50") +
  
  scale_x_log10(labels = label_dollar(scale_cut = cut_short_scale())) +
  scale_y_continuous(labels = label_dollar(scale_cut = cut_short_scale())) +
  
  labs(
    title = "Residuals vs. Fitted Values",
    x = "Predicted Revenue (Fitted)",
    y = "Residual Error (Actual - Predicted)"
  ) +
  theme_minimal(base_size = 18)

```

## Conclusion

- Across all models, the strongest and most consistent predictors were:  
  **votes, company frequency, director + star popularity, runtime, and rating categories**
- **Linear Regression:** clear interpretability  
  **R² ≈ 0.59 · RMSE ≈ \$42M**
- **Elastic Net:** stable model, removes noise predictors  
- **LASSO:** emphasized star power as a major driver  
  **RMSE ≈ \$38.5M**
- **Tree Models:** 
  Relatively expensive to run for marginal gains. Struggles with small sample size
  **RMSE 38.9M R² ≈ 0.59**
- **Overall:** Models agree—**movie popularity + industry influence** (stars, directors, production reputation) are key to predicting revenue.



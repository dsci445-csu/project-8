---
title: "Predicting Movie Gross Revenue"
subtitle: "DSCI 445"
author: "Group 8: Kelsey Britton, James Chinnery, Robin Thrush, Kaitlynn Walston"
output: 
  ioslides_presentation:
    css: styles.css
---

```{r setup, include=FALSE, message = FALSE}
set.seed(445)
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)
library(tidymodels)
library(knitr)
library(kableExtra)
library(dplyr)
library(ggplot2)
clean_data <- read_csv("clean_movie_data.csv")
source("linreg.R")
```

```{r robin - code I need for the plots to work, include = FALSE}
clean_data <- read_csv("clean_movie_data.csv") 
clean_data <- clean_data |>
  select(-c("X", "country", "director", "name", "writer", 
            "company", "star", "id", "budget_missing", "year", 
            "release_month", "writer_count", "director_count", 
            "direc_freq","star_count"))
clean_data <- clean_data |>
              mutate(rating = ifelse(rating %in% c("UNRATED", "NOT RATED", 
                                                   "Not specified"), 
                                     "Unknown", rating))
clean_data <- clean_data |>
  mutate(gross = log(gross + 1))
net_recipe <- recipe(gross ~ ., data = clean_data) |>
  step_mutate(budget = log(budget+1),
              votes = log(votes +1),
              comp_freq = log(comp_freq + 1),
              runtime = log(runtime + 1),
              score = log(score + 1),
              writer_popularity = log(writer_popularity + 1),
              director_popularity = log(director_popularity +1),
              star_popularity = log(star_popularity + 1)) |>
  step_novel(all_nominal_predictors()) |> ## takes care of factor lvls
  step_unknown(all_nominal_predictors()) |> ## handles missing vals. 
  step_dummy(rating, genre, region, season) |> ## categorical vars.
  step_zv(all_predictors()) |> ## removes zero-variance col.
  step_normalize(all_numeric_predictors()) ## standardizes
elast_net <- linear_reg(penalty = tune(), mixture = tune()) |>
  set_engine("glmnet") 

net_workflow <- workflow() |>
  add_recipe(net_recipe) |>
  add_model(elast_net)
folds <- vfold_cv(clean_data, v = 10)
grid_net <- grid_regular(penalty(),
                         mixture(range = c(0,1)),
                         levels = 20)
the_metrics <- metric_set(rmse, rsq, mae)
tuned_results <- tune_grid(net_workflow,
                           resamples = folds,
                           grid = grid_net,
                           control = control_grid(save_pred = TRUE),
                           metrics = the_metrics)
best_parameters <- tuned_results |>
  select_best(metric = "rmse")
final_net <- finalize_workflow(net_workflow, best_parameters)
final_fit <- fit(final_net, data = clean_data)

preds <- predict(final_fit, clean_data) |>
  bind_cols(clean_data)
glm_fit <- extract_fit_parsnip(final_fit)$fit
lambda_used <- best_parameters$penalty

coefs <- coef(glm_fit, s = lambda_used)
coef_dense <- as.matrix(coefs)
non_z_co <- coef_dense[coef_dense[,1] != 0 & 
                         rownames(coef_dense) != "(Intercept)", , drop = FALSE]
coef_df <- data.frame(predictor = rownames(non_z_co),
                      coefficient = non_z_co[,1])
```

## Introduction

Dataset: IMDb movie data (1986–2016), including budget, cast, ratings, genre, and more  
- Goal: Build models that **predict a movie’s gross revenue**  
- Approach:
  - Each group member cleaned specific variables  
  - Combined into one final dataset  
  - Applied multiple modeling methods:
      - Linear Regression  
      - LASSO  
      - Elastic Net  
      - Bagged & Boosted Trees  
**Focus: Which predictors influence revenue the most, and which model performs best?**

## Data Cleaning

**Date**

  - Extracted month from "date" column 
  - Used this to assign each entry a season (Fall, Winter, Spring, Summer)
  
**Movie Title**

  - Sentiment analysis using AFINN
  - Added Punctuation

## Pure Linear Regression

- Used cleaned predictors from compiled csv
- Fit on log(gross) scale for stability
- Predictions were coverted back on dollars scale

## Model Performance

**$R^2$ = 0.594**

**RMSE = $42.3 million**

**MAE = 19043383**

## 

```{r}
significant_table <- tidy(lm_full) %>%
  filter(p.value < 0.05) %>%
  mutate(
    pct_impact = round((exp(estimate) - 1) * 100, 2),
    p.value = signif(p.value, 3)
  ) %>%
  arrange(desc(abs(pct_impact))) %>%
    select(
    Predictor = term,
    `Percent Impact on Gross` = pct_impact,
    `p-value` = p.value) 

top10_table <- significant_table %>%
  slice(1:10)
  
top10_table %>%
  kable(format = "html", caption = "Most Significant Predictors") %>%
  kable_styling(full_width = FALSE)
```

##

```{r}

ggplot(plot_df, aes(x = Actual, y = Predicted)) +
  geom_point(alpha = 0.3) +
  geom_abline(intercept = 0, slope = 1, color = "red") +
  scale_x_continuous(labels = scales::dollar) +
  scale_y_continuous(labels = scales::dollar) +
  labs(
    title = "Predicted vs Actual Gross Revenue",
    x = "Actual Gross (USD)",
    y = "Predicted Gross (USD)"
  ) +
  theme_minimal()
```



## Method & Motivation: 
**Elastic Net** 

- Reminder:
Elastic Net is a shrinkage model that combines LASSO and Ridge Regression
penalties.

- Why Elastic Net:

  - Large amount of predictors that aren't meaningful are dropped
  
  - Handles collinearity well like assuming comp_freq and director popularity
  
  - Able to rank predictors
  - Stabilize coefficients of correlated predictors

Since our primary goal was to identify strongest predictors, we did not remove possible correlated terms and did not create interaction terms. 


## Handling Data: 

- How: 

** Log transformed gross (response variable) 

** Log +1 transformed all numeric predictors

** Dummy encoded the categorical variables: Genre, Region, Rating and Season

10 fold cross-validation 

## Hyperparameter Tuning:

Grid of 400 different combos of alpha and lambda
Each combo used to compute metrics for RMSE, MAE, and R^2

The best RMSE (lowest CV RMSE) was pulled out and revealed the ideal lambda and 
alpha values. 

Finally we are able to pass this back to the workflow and come up with the best 
elastic net model. 

**Cross Validation:**

Best Lambda: 0.02636651

Best Alpha: 0.4736842

RMSE: 1.575973  

R^2: 0.584

MAE: 1.13

```{r MAECV}
exp(1.13)
```


## Prediction Model: 

**Prediction:**

RMSE: 1.564085

R^2 : 0.5909

MAE: 1.120881

```{r MAEPred}
exp(1.12)
```

The RMSE from cross-validation (1.57) and RMSE from prediction (1.56) are very 
close, indicating the model is stable and not overfitting.

```{r avg prediction err}
1.564085/mean(clean_data$gross)
```

The RMSE relative to the average gross is abt. 10%. AKA the 
model predicts movie revenue with roughly +/- 10% accuracy.

##

```{r actual v pred visual}
act_pred <- ggplot(data = preds, mapping = aes(x = gross, y = .pred)) +
  geom_point(alpha = 0.2) +
  geom_abline(intercept = 0, slope = 1, color = "darkred") + 
  labs(title = "Actual VS Predicted Gross",
       x = "Actual Log Gross",
       y = "Predicted Log Gross") 
act_pred
```

## Predictors:

Total predictors: 53

Leftover: 39 and Shrunk to Zero: 14

**Strongest predictors:**
- Votes (0.966)
- comp_freq (0.444)
- rating_Unknown (-0.441)

**Moderate Predictors:**
- Director_popularity (0.225)
- Runtime (0.179)
- Budget (0.113)


**Predictors that shrunk to 0:**
- Sentiment
- Oceania
- Genre: Musical

##

```{r predictor barplot}

coef_df <- coef_df |>
  mutate(abs_coef = abs(coefficient)) |>
  arrange(desc(abs_coef))

top_coef <- coef_df |>
  filter(rank(-abs_coef) <= 15)

bar_pred <- ggplot(data = top_coef, aes(x = reorder(predictor, abs_coef), 
                                        y = abs_coef, fill = coefficient > 0)) + 
  geom_col() + 
  coord_flip() + 
  scale_fill_manual(values = c("darkred", "darkgreen"), 
                    labels = c("Negative", "Positive")) +
  labs(title = "Predictor Importance",
       x = "Predictor",
       y = "Coefficient Magnitude",
       fill = "Coefficient Sign")
bar_pred
```


## Elastic Net Takeaway:

- Identified important predictors
- Predicts movie gross within ~10% of the actual gross
- High-gross movies are predicted more accurately than low-gross movies
- Demonstrates stability as it performs very similarly between cross-validation
and full-data predictions. 


## LASSO Model: 

```{r lasso}
# data
ds <- read.csv("clean_movie_data.csv")
df <- na.omit(ds)

top_stars <- names(sort(table(df$star), decreasing=TRUE))[1:50]
df$star <- ifelse(df$star %in% top_stars, df$star, "Other")
df$star <- as.factor(df$star)

# response var
y <- df$gross

# predictors 
predictors <- df %>%
  select(gross, budget, runtime, year, score, rating, genre, star, 
         season, budget_missing, direc_freq, director_count, director_popularity,
         comp_freq, writer_count, writer_popularity, star_count, star_popularity,
         sentiment_best)
predictors <- predictors %>% select(-gross)

# model matrix (dummy coding)
X <- model.matrix(~ ., data=predictors)[, -1]

# cv lasso
cv_lasso <- cv.glmnet(x=X, y=y, alpha=1, nfolds = 10, standardize = TRUE)
lamb_min <- cv_lasso$lambda.min
lamb_1se <- cv_lasso$lambda.1se

# coefs at lasso min
coef_lasso <- coef(cv_lasso, s="lambda.min")
coef_lasso <- coef_lasso[coef_lasso != 0]

pred <- predict(cv_lasso, newx=X, s="lambda.min")

coef_clean <- coef(cv_lasso, s = "lambda.min") %>%
  as.matrix() %>%
  as.data.frame() %>%
  rownames_to_column("term") %>%
  rename(coefficient = 2) %>%
  filter(coefficient != 0) %>%
  mutate(
    variable = case_when(
      term == "(Intercept)" ~ "(Intercept)",
      str_starts(term, "star") ~ "star",
      str_starts(term, "genre") ~ "genre",
      str_starts(term, "rating") ~ "rating",
      TRUE ~ term         # numeric variables
    ),

    # Extract category level (only for factors)
    level = case_when(
      variable %in% c("star", "genre", "rating") ~ 
        str_remove(term, paste0(variable)),
      TRUE ~ NA_character_
    ),
    level = str_trim(level),

    coefficient = round(coefficient, 2),
    abs_coef = round(abs(coefficient), 2),

    interpretation = case_when(
      variable == "(Intercept)" ~
        "Baseline predicted gross at all reference categories.",
      
      variable == "budget_missing" ~
        "Difference in gross when the budget was missing.",
      
      variable %in% c("star","genre","rating") ~
        paste0("Effect of category '", level, 
               "' relative to the reference ", variable, "."),

      TRUE ~ paste0("Effect of a 1-unit increase in ", variable, ".")
    )
  ) %>%
  select(variable, level, coefficient, abs_coef, interpretation) %>%
  arrange(desc(abs_coef))

coef_clean

# rmse
pred <- predict(cv_lasso, newx=X, s="lambda.min")
rmse <- sqrt(mean((y-pred)^2))
cat("LASSO rmse: $",rmse)
```
*Results*

- The baseline/intercept we are given is a gross of ~554 million dollars. 
- The coefficients we see are based on the income compared to the baseline. For example, Tom Hanks would be predicted to earn about 28.4 million dollars more than the assumed baseline, but Nicole Kidman would be predicted to make about 27.2 million dollars less than the baseline.
- This lasso model identified leading actors was the most influential predictor
- RMSE = 38.5 million dollars, meaning that on average, this lasso model's predictions for movie revenue is off by about 38.5 million dollars






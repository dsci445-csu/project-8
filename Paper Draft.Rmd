---
title: "Group 8 DSCI445 Paper DRAFT"
author: |
  Group 8  
  Kelsey Britton · James Chinnery · Robin Thrush · Kaitlynn Walston
output:
  pdf_document
fontsize: 11pt
geometry: margin=1in
---

```{r seed}
set.seed(445)
```

```{r libraries, echo = FALSE, message= FALSE}
library(broom)
library(tidyverse)
library(dplyr)
library(readr)
library(readxl)
library(ggplot2)
library(tidyr)
library(tidyverse)
library(glmnet)
library(tree)
library(randomForest)
library(caret)
library(rsample)
library(tidymodels)
library(workflows)
library(recipes)
library(glmnet)
library(yardstick)
library(Matrix)
```

```{r setup, echo = FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  message = FALSE,
  warning = FALSE,
  fig.align = "center",
  fig.width = 7,
  fig.height = 5
)
```


```{r elastic net, echo = FALSE}
clean_data <- read_csv("clean_movie_data.csv") 

## removing columns that are repeats/not predictors (id)
clean_data <- clean_data |>
  select(-c("X", "country", "director", "name", "writer", 
            "company", "star", "id", "budget_missing", "year", 
            "release_month", "writer_count", "director_count", 
            "direc_freq","star_count"))

## Collapsing rating, more digestible, better for dummy encoding
clean_data <- clean_data |>
              mutate(rating = ifelse(rating %in% c("UNRATED", "NOT RATED", 
                                                   "Not specified"), 
                                     "Unknown", rating))
## log transforming gross, helping w skew
clean_data <- clean_data |>
  mutate(gross = log(gross + 1))
## net recipe, and making sure the other numeric values are log transformed
## to maintain consistency w gross.
## all the steps are to ensure glmnet works
net_recipe <- recipe(gross ~ ., data = clean_data) |>
  step_mutate(budget = log(budget+1),
              votes = log(votes +1),
              comp_freq = log(comp_freq + 1),
              runtime = log(runtime + 1),
              score = log(score + 1),
              writer_popularity = log(writer_popularity + 1),
              director_popularity = log(director_popularity +1),
              star_popularity = log(star_popularity + 1)) |>
  step_novel(all_nominal_predictors()) |> ## takes care of factor lvls
  step_unknown(all_nominal_predictors()) |> ## handles missing vals. 
  step_dummy(rating, genre, region, season) |> ## categorical vars.
  step_zv(all_predictors()) |> ## removes zero-variance col.
  step_normalize(all_numeric_predictors()) ## standardizes
  
## elastic net model w ridge and lasso penalties, tune() being chosen
elast_net <- linear_reg(penalty = tune(), mixture = tune()) |>
  set_engine("glmnet") 

## puts all the steps together 
net_workflow <- workflow() |>
  add_recipe(net_recipe) |>
  add_model(elast_net)


## splits data into train and test 10 fold cross valdation time. 
folds <- vfold_cv(clean_data, v = 10)

## range for potential lambda and alpha, 10 evenly spaced pts.
grid_net <- grid_regular(penalty(),
                         mixture(range = c(0,1)),
                         levels = 20)

## trying different lambda and alpha --> hyperparameters
## penalty (lambda) controls shrinkage
## mixture (alpha) controls type of shrinkage 
##   alpha = 1 is lasso alpha = 0 is ridge and anything in between is elastic net :D

the_metrics <- metric_set(rmse, rsq, mae)
tuned_results <- tune_grid(net_workflow,
                           resamples = folds,
                           grid = grid_net,
                           control = control_grid(save_pred = TRUE),
                           metrics = the_metrics)

## So we know what is being selected
select_best(tuned_results, metric = "rmse")


## finding best lambda and alpha
best_parameters <- tuned_results |>
  select_best(metric = "rmse")

cv_mets <- collect_metrics(tuned_results)
mae_CV <- cv_mets |>
  filter(.metric == "mae",
         near(penalty, best_parameters$penalty),
         near(mixture, best_parameters$mixture))

r_sq <- cv_mets |>
  filter(.metric == "rsq",
         near(penalty, best_parameters$penalty),
         near(mixture, best_parameters$mixture))

## only using the best parameters found
final_net <- finalize_workflow(net_workflow, best_parameters)

## resultt
final_fit <- fit(final_net, data = clean_data)

preds <- predict(final_fit, clean_data) |>
  bind_cols(clean_data)

#mae(preds, truth = gross, estimate = .pred)

## want rmse and rsq, indicative of quality of prediction model
#rmse(preds, truth = gross, estimate = .pred)

#rsq(preds, truth = gross, estimate = .pred)

## The actual fitted glmnet model that contains all the coefficients, ignores the workflow stuff
glm_fit <- extract_fit_parsnip(final_fit)$fit

## best lambda again
lambda_used <- best_parameters$penalty

## coefficients of the fitted model w the best lambda
coefs <- coef(glm_fit, s = lambda_used)

## matrix of the predictors and their coefficients
coef_dense <- as.matrix(coefs)

## Removes any predictors that had the value 0 for their coefficients and removes Intercept coefficient
non_z_co <- coef_dense[coef_dense[,1] != 0 & rownames(coef_dense) != "(Intercept)", , drop = FALSE]

## Putting it into a dataframe
coef_df <- data.frame(predictor = rownames(non_z_co),
                      coefficient = non_z_co[,1])

predic_vals <- predict(final_fit, clean_data)$.pred
```


# Introduction

Our group chose to work with movie data and our goal is to build a predictive model that predicts the gross revenue that a movie makes. The dataset was pulled from IMDB and is filled with movies made in 1986 to 2016. Some variables included in this dataset are budget, star actor, director, rating, and genre. To make the data and interpretation more digestible, each of us first worked with three predictor variables and saw how they each affected the gross of the movie. Then, we came together and discussed the results and what changes we had to make to the data to build a model. We combined the ways we changed the data to have a final cleaned dataframe. We then each took on a different method that would inform us of which predictors are the most influential to predicting a movie’s gross revenue using that final dataframe and discussed the results we got. 

# Methods

### Kelsey: 
The variables that I cleaned were release date and movie title. The original release date column had multiple inconsistent format types, so I extracted just the month as a numerical value. I also used this value to assign each entry a release season (March, April, May as Spring, June, July, August as Fall, etc.). For the movie’s title I used sentiment analysis to assign each entry a sentiment score. This was done by combining AFINN word sentiment with some additional scoring adjustments to account for punctuation to allow titles with “?” and “!” to reflect a stronger emotional tone in either direction. 

### James:
 Budget was the main column I worked on. Since over a third of the data was missing I chose to flag it and fill it with the median value for that year. In theory this should allow the models to still use the data from those entries and simply trust the budget less. For the runtime of the movies it was numeric and well behaved so it was left alone. The writers were initially lumped into 5 categories. This was somewhat arbitrary though and a better approach was to lump into two categories since so many writers only have one movie then this threshold could be tuned  though I this was changed for some of the methods as they were better equipped to handle target encoded data 

### Robin:
 The variables I worked with were director, company, and country. They are all characters, and unfortunately too unique to rely on the standard dummy encoding. To navigate this, I decided to assign countries to regions, and allow for regions to be dummy encoded because there were way less unique values. When doing this the function automatically made “Africa” the baseline region which caused the other regions to have an unusual standard for the regions and their influence on gross. Africa only had 10 movies associated with it, and all the movies had relatively high gross, and so to fix this I changed the baseline region to be North America which had 5040 movies associated with it, and a much wider range for gross. For the director and company I decided to use frequency encoding which allowed for the data to be digestible for a linear regression model. The caveat with using frequency encoding is that if there are directors that have the same frequency, they will be treated identically, despite being associated with different movie revenues. I transformed the data using log1p() which was supposed to help with dealing with skew, especially with the variables that had an extremely large range (gross, budget, etc). I fit a full model with the director, company, and region. When log transforming data, you interpret the results as how the percent change in the variable relates to the percent change in gross. So for example a 1% increase in director frequency is associated with a 0.807% increase in gross with all other things held constant. The full model with these three variables revealed that director seemed to have more influence than the company or region. I also fit a residual vs fitted plot and QQ plot which revealed that non-constant variance and that the model seemed to be more consistent in predicting high-gross movies than mid to low gross movies. QQ plot mostly normal, left tail was slightly curved, so some non-normality was going on. Overall, okay model with just the three variables, but likely could be improved with the inclusion of other predictors.

### Kaitlynn: 
The variables I worked with were genre, rating, and stars. Since they were all categorical, I first started out by creating frequency tables to visually see how much each category occurs. After that I cleaned the data by using dummy coding to better represent the categorical variables. The goal, after cleaning the data was, to use the LASSO to help us identify which of the variables have the strongest influence on gross revenue while reducing the noise from the other, less impactful predictors. So, using the cleaned data, I created a LASSO model to try to predict the gross revenue as well as showed the results in a table in order to show the top impactful predictors. 

### Together: 
All of these cleaning methods were combined into a single csv file for use in the modeling.

# Results:

### Kelsey: Pure Linear Regression
 For the pure linear regression model, all variables were standardized and log-transformed for comparability. The model explained 59% of the variation in log-transformed gross revenue (R-Squared = 0.594) and the RMSE was 1.56. Among the variables, the vote count on IMDb, the frequency at which the production company popularity (denoted by the frequency they appeared in the data set), star popularity, and runtime were all highly significant positive predictors. The genre being Drama, the genre being Crime, and the IMDb score had the most significant negative effect on gross income. In total, there were 23 significant predictors within the model, encompassing notoriety, runtime, IMDb score, region, genre, region, budget, release season and rating. Overall, the linear regression model identifies strong, interpretable relationships between some movie attributes and their gross income.
	
### Kaitlynn: LASSO
For the LASSO model, the predictors that impacted the prediction of gross revenue was the movie stars, budget, and genre, having the movie stars the most impactful. The LASSO gave us a baseline predicted gross revenue, holding constant the other predictors, and that baseline was set at gross of \$554 million dollars. We can see that the top coefficients were different popular movie stars. Some of the movie star examples that we could see in the model results were: Tom Hanks, Nicole Kidman, Sandra Bullock, and Johnny Depp. Actors like Tom Hanks and Sandra Bullock earned about \$28.45 and \$23 million over the baseline respectively, however we can also see actors like Johnny Depp and Nicole Kidman are predicted to earn about \$16.4 and \$27.2 million less than the baseline. Thus showing that depending on who stars in your movie would greatly impact the gross revenue of the movie. The budget_missing indicator also had a noticeable effect ont he gross revenue prediction, since it was able to note that the films without reported budgets tended to earn substantially less than those with reported budgets.

### Robin: Elastic Net
After group data cleaning, we had 1 comprehensible dataframe with the predictors we would need to run shrinkage and prediction modeling. After cleaning we were still left with 4 columns that were categorical variables: genre, rating, region, and season. To handle this I used dummy encoding, and the model would select the most informative reference categories. I also removed similar column predictors, for example we had director frequency, director count, and director popularity. I treated them as the same and decided to drop director frequency and director count and only used director popularity in my model. Other variables that were handled similarly included writer and star. The remaining predictors were numerical and I log transformed them using log(x +1) to address skewness and handle zero values. To build the model I utilized a bunch of R packages like recipe, workflow, and tune to standardize variables, select the lambda and alpha values, etc. The data was split using 10-fold cross validation, splitting the data 90-10 for training and testing.

A grid of 400 different combinations for lambda (regularization) and alpha (mixture) was created and then the metrics rmse, mae, and rsq for each combination was calculated. The group decided to measure the model's performance using rmse. I was able to pull the associated lambda and alpha which were 0.0263 and 0.4737 respectively. The lambda is small, which indicates weak regularization. The elastic net model could be retaining too many predictors, causing potential overfit. The alpha was almost 0.5 which would have been a perfect mix of the LASSO and ridge penalties. 

Then the "best" elastic net model was fit using the values we found for lambda and alpha. A prediction model was made inspired by the final fit. For comparison to see how the predict model performed we measured up the metrics we collected earlier rmse, mae, and rsq.
From these findings we were able to tell that the predictive model performed well. The rmse value from the training and the test rmse were just about the same, meaning that the model generalized well (38.5 million and 37.7 million respectively) The training mae and the test mae are a bit further apart in value, but they are similar enough, which just corroborates the conclusion that the model generalized well (19.3 million and 18.4 million respectively). 

Lastly, to identify the most influential predictors, I isolated the predictors and their coefficients. Filtered out the ones that were dropped to zero, and ordered the ones that were leftover by their magnitude. This is because there were predictors that were associated with changes in achieving higher or lower gross. Though we want to know which predictors are the most influential in predicting higher-gross, it was interesting to see the ones that had a negative influence (like having an "unknown" rating). The elastic net model went reduce the original 53 predictors to 39. 14 were dropped to zero. The top three predictors that were the most influential for higher-gross were: votes, company frequency, and movies that were made in North America. 


### James: Tree Boosted/ Bagged Random
I approached the tree based models with a slightly different approach, since tree based models theoretically choose the ideal point to split the data at each node I wanted to give them the chance to choose this point rather than relying on the arbitrary splits we created with the frequency based binning approach outlined in the methods section. To this end I initially fit the tree based models using target encoded data. This kept the dimensional low and hopefully added some extra information for the ensemble models to pick up on. This did risk over fitting because the target encoding can cause problems when there are a low number of samples making the target value just the value. Despite this I thought the trees would be resistant to this for a few reasons. The package used to do the target encoding has smoothing factor to trust the means of poorly represented features less. In addition to that in theory the ensemble  based methods should be highly resistant to  over fitting in this manner by nature of being the average of many tree. Finally all hyper parameters were tuned using 5 fold CV. 5 Folds were chosen for training speed since these models ran somewhat slowly. I hoped the combination of these techniques would reduce over fitting to an acceptable level.  
Using this method I fit both a random forest and a boosted tree. The Random Forest was tuned for the minimum data points required for a node split as well as the number of predictors sampled for the sub trees. Similarly the boosted tree was also tuned on the minimum points for a node split, in addition to both depth and learn rate.Both the Forest and Boosted Tree were tuned on 100 trees. This number was fixed for speed and only used for tuning. The final model was fit with 1000 trees, I decided to push it high for the final fit as these models don't really change if there are too many trees instead just running slowly this was acceptable for the final model given it was only a single model being fit. 

The tree based methods achieved and rsq of .508 and a rmse of roughly 40 million in real dollars. Its important to not place too much importance on the rmse here though. By nature the biggest movies will dwarf all the others in terms of revenue which manifests as highly left skewed which has a large impact on the mean. Despite this rsme was used as the metric for two reasons. It is a standard for most regression problems due to its smooth characteristics and in an environment like movies theres far more theoretical upside to predicting the next big hit as opposed the next middle of the pack title. The direct result of that is an rmse that is highly influenced by the massive outliers at the top even after the log transformation which makes the number itself somewhat unreliable. Instead one can focus on the rsq which says that 50% variance is explained by our model. Its not amazing but for financial data rather respectable for an ensemble model. The reason I mention that ensemble models as a class in general is because the struggle with monotonic relationships seen in financial data. Its a reasonable expectation that spending more on a movie should increase revenue. This isn’t always the case but the general idea it better captured by linear modeling approaches. This is seen in the superior performance of the elastic net 

## Figures



The figure below is the actual versus the predicted plot of the elastic net model.
We can see that it is more accurate in predicting higher gross films.
```{r actual v predicted}

act_pred <- ggplot(data = preds, mapping = aes(x = gross, y = .pred)) +
  geom_point(alpha = 0.2) +
  geom_abline(intercept = 0, slope = 1, color = "darkred") + 
  labs(title = "Actual VS Predicted Gross",
       x = "Actual Log Gross",
       y = "Predicted Log Gross") 
act_pred
```


The barplot below shows how influential each predictor was for films gross.
This visual is useful for justifying our final claim that audience engagement and
industry reputation are most important for high gross films. 

```{r barplot predictors magnitude}

coef_df <- coef_df |>
  mutate(abs_coef = abs(coefficient)) |>
  arrange(desc(abs_coef))

top_coef <- coef_df |>
  filter(rank(-abs_coef) <= 15)

bar_pred <- ggplot(data = top_coef, aes(x = reorder(predictor, abs_coef), y = abs_coef, fill = coefficient > 0)) + 
  geom_col() + 
  coord_flip() + 
  scale_fill_manual(values = c("darkred", "darkgreen"), labels = c("Negative", "Positive")) +
  labs(title = "Predictor Importance",
       x = "Predictor",
       y = "Coefficient Magnitude",
       fill = "Coefficient Sign")


bar_pred
```



## Conclusion/Changes if we did it again:

Across all modeling approaches a consistent set of predictors were revealed as the most influential in determining gross. This would be votes, company frequency, director popularity, and star popularity. In terms of performance, we used RMSE as our measure of comparing models. Linear Regression came in at the highest RMSE, and following was Tree-based models, LASSO, and Elastic Net. Their respective RMSEs being \$--, \$--, \$--, and \$37.7 million. Notably, despite using a variety of different approaches the RMSE's and R^2 values all clustered around similar values. This points to the data being the main culprit behind the relatively poor results of the models. This makes sense in practice because on account of how many people may only show up on the list once in six thousand entries leading to points where theres really no trend to learn as there's only one example. The end result is data where the true trends have a very high amount of erogenous information that essentially amounts to noise. The only fix here really is more data which given the finite number of movies isn't very feasible. 

Despite the different approaches we took, the models agreed on the most influential predictors. We can generalize the strongest predictors as: audience engagement and industry reputation. For real world application we could say that for films to be financially successful they should heavily advertise, cast popular actors, and have known directors direct the film. This result isn't particularly surprising through notable it should be mentioned how how of the most impact predictors are really just side effects of raising the budget. Getting a household name in a given movie isn't cheap in most cases. That said we didn't see this correlation with budget directly, leading to the conclusion that although the best way to make a hit movie is expensive, money alone is not enough. It gives insight into how film makers should allocate their budget to ensure box office success.

If we were to continue this work there are some notable gaps that could be addressed. Notably our dataset lacked data on marketing effort as well as how many theaters a movie played in. This data could potentially help distinguish between films that just couldn't be a massive commercial success because they outright lacked the potential audience. In addition to that getting more data on the budget of movies could substantially help our model. Movies with a budget of 0 do exist and by employing a blanket policy of just setting them all to the median there was some data lost that may have been useful in predicting lower performing films. Finally, though difficult theres lots of potential in finding a way to quantify to encode the plot of the movie. Though we had features that helped quantify how good a movie actually was, there was no way to tell if the plot of a movie had mainstream appear perhaps sentifment analyis of a summary of the movie could provide some of this insight  



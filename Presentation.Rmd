---
title: "Predicting Heart Failure from Risk Factor Data"
author: "Paige Galvan, Neha Deshpande, & Witlie Leslie"
date: "2024-11-25"
output:
  beamer_presentation: default
  powerpoint_presentation: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

# Introduction (Witlie)

For our project, we aimed to create a model that can predict the event of death as a result of heart failure from clinical patient data.

- Binary response variable "DEATH_EVENT", value of 1 indicating patient deceased (0 otherwise)
- 12 risk factor variables
    - 5 binary: anemia status, diabetes status, high blood pressure status, sex, and smoking status
    - 7 numerical: age, creatine phosphokinase level, ejection fraction, platelet concentration, serum creatine level, serum sodium level, and length of follow-up period

# Motivation (Neha)

- Investigating heart failure, a condition impacting millions worldwide, and exploring its complex causes and contributing factors.

- Spotting patterns between risk factors and how heart failure progresses to get a better understanding.


# Linearity and Normality (Neha)


# Logistic Regression (Witlie)

Logistic regression was an obvious first choice in tackling this binary classification problem. I created 3 models to assess predictive performance with different regularization methods.

- Ridge Regression
- Lasso
- No regularization

# Ridge Regression

```{r, echo=FALSE, warning=FALSE, message=FALSE}
set.seed(445)

library(tidymodels)
library(tidyverse)
library(ggplot2)
library(dplyr)

data <- read.csv("heart_failure_clinical_records_dataset.csv")

# scale continuous variables
numeric_vars <- sapply(data, is.numeric)
boolean_vars <- sapply(data, function(x) all(x %in% c(0,1)))

need_scale <- numeric_vars & !boolean_vars

data[need_scale] <- scale(data[need_scale])

# convert DEATH_EVENT to a factor 
data <- data %>%
  mutate(DEATH_EVENT = factor(DEATH_EVENT, levels = c("0", "1")))

data_split <- initial_split(data, prop = 0.8)
train_data <- training(data_split)
test_data <- testing(data_split)

prep_data <- recipe(DEATH_EVENT ~ ., data = train_data)

# create vector of 100 lambda values from 0.01 to 10^10
lambda <- lambda <- 10^seq(-2, 10, length.out = 100)
tune_df <- data.frame(lambda = lambda)

# fit ridge regression model for each lambda
prep_data <- recipe(DEATH_EVENT ~ ., data = data)

ridge_ests <- data.frame()

for(lam in lambda) {
  ridge_spec <- logistic_reg(mixture = 0, penalty = lam) |>
    set_mode("classification") |>
    set_engine("glmnet")
  
    workflow() |>
    add_model(ridge_spec) |>
    add_recipe(prep_data) |>
    fit(data) |>
    tidy() |>
    bind_rows(ridge_ests) -> ridge_ests
}

# visualize estimated coefficients for each lambda
ridge_ests |>
  filter(term != "(Intercept)") |>
  ggplot() +
  labs(title = "Ridge Regression: Coefficient Estimates vs Penalty") +
  geom_line(aes(penalty, estimate, group = term, colour = term)) +
  coord_trans(x = "log10")
```

# Ridge Regression

Using 10-fold cross-validation, I found the lambda with the highest ROC-AUC value of 0.887 was lambda = 0.0933

```{r, echo=FALSE, warning=FALSE, message=FALSE}
# perform 10-fold cross validation, estimate roc_auc for each lambda
data_10foldcv <- vfold_cv(data, v = 10)

ridge_spec <- logistic_reg(mixture = 0, penalty = tune("lambda")) |>
  set_mode("classification") |>
  set_engine("glmnet")

workflow() |>
  add_model(ridge_spec) |>
  add_recipe(prep_data) |>
  tune_grid(resamples = data_10foldcv, grid = tune_df, metrics = metric_set(roc_auc, accuracy)) -> ridge_tune

# determine best lambda

ridge_metrics <- ridge_tune |>
  collect_metrics() |>
  filter(.metric == "roc_auc") |>
  mutate(method = "Ridge")

ridge_best <- show_best(ridge_tune, metric = "roc_auc", n = 1)

ridge_tune |>
  collect_metrics() |>
  dplyr::select(lambda, .metric, mean) |>
  pivot_wider(names_from = .metric, values_from = mean) |>
  ggplot() +
  labs(title = "Ridge Regression: roc_auc vs lambda") +
  geom_line(aes(lambda, roc_auc)) + 
  geom_point(aes(lambda, roc_auc)) +
  geom_point(aes(ridge_best$lambda, ridge_best$mean), color = "red", shape = 1, size = 4) +
  coord_trans(x = "log10")
```

# Lasso

Lasso shows a steeper dropoff of coefficient estimates as a result of feature reduction

```{r, echo=FALSE, warning=FALSE, message=FALSE}
# fit lasso model for each lambda
lasso_ests <- data.frame()
for(lam in lambda) {
  lasso_spec <- logistic_reg(mixture = 1, penalty = lam) |>
    set_mode("classification") |>
    set_engine("glmnet")
  
    workflow() |>
    add_model(lasso_spec) |>
    add_recipe(prep_data) |>
    fit(data) |>
    tidy() |>
    bind_rows(lasso_ests) -> lasso_ests
}

# visualize estimated coefficients for each lambda
lasso_ests |>
  filter(term != "(Intercept)") |>
  ggplot() +
  labs(title = "Lasso Method: Coefficient Estimates vs Penalty") +
  geom_line(aes(penalty, estimate, group = term, colour = term)) +
  coord_trans(x = "log10")
```

# Lasso

Using 10-fold cross validation, I found the lambda with the highest ROC-AUC value of 0.886 was lambda = 0.01

```{r, echo=FALSE, warning=FALSE, message=FALSE}
# perform 10-fold cross validation, estimate roc_auc for each lambda
lasso_spec <- logistic_reg(mixture = 1, penalty = tune("lambda")) |>
  set_mode("classification") |>
  set_engine("glmnet")

workflow() |>
  add_model(lasso_spec) |>
  add_recipe(prep_data) |>
  tune_grid(resamples = data_10foldcv, grid = tune_df, metrics = metric_set(roc_auc, accuracy)) -> lasso_tune

lasso_metrics <- lasso_tune |>
  collect_metrics() |>
  filter(.metric == "roc_auc") |>
  mutate(method = "Lasso")

lasso_best <- show_best(lasso_tune, metric = "roc_auc", n = 1)

# show best lambda with lasso
lasso_tune |>
  collect_metrics() |>
  dplyr::select(lambda, .metric, mean) |>
  pivot_wider(names_from = .metric, values_from = mean) |>
  ggplot() +
  geom_line(aes(lambda, roc_auc)) +
  labs(title = "Lasso Method: ROC-AUC vs Lambda") +
  geom_point(aes(lambda, roc_auc)) +
  geom_point(aes(lasso_best$lambda, lasso_best$mean), color = "red", shape = 1, size = 4) +
  coord_trans(x = "log10")
```

# Results

- No Regularization:

- Ridge Regression:

- Lasso:


# SVM (Paige)

- Predicting Death Event 
- Like our Linear Regression methods we began by splitting the data into training and test data. 
- I began by assessing the support variables and determining the acurracy of the model. 

# Random Forest (paige)



# Results (Paige)

# References 

Davide Chicco, Giuseppe Jurman: Machine learning can predict survival of patients with heart failure from serum creatinine and ejection fraction alone. BMC Medical Informatics and Decision Making 20, 16 (2020). https://bmcmedinformdecismak.biomedcentral.com/articles/10.1186/s12911-020-1023-5 

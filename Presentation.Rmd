---
title: "Predicting Heart Failure from Risk Factor Data"
author: "Paige Galvan, Neha Deshpande, & Witlie Leslie"
date: "2024-11-25"
output:
  beamer_presentation: default
  powerpoint_presentation: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```


```{r, include = FALSE}
library(caTools) 
library(randomForest) 
library(e1071) 
library(ggplot2)
library(dplyr)
library(tidyr)
library(readr)
library(MASS)
library(GGally)
library(tidymodels)
library(ISLR)
library(class)
library(reshape2)
library(pROC)
```

## Introduction (Witlie)

For our project, we aimed to create a model that can predict the event of death as a result of heart failure from clinical patient data.

- Binary response variable "DEATH_EVENT", value of 1 indicating patient deceased (0 otherwise)
- 12 risk factor variables
    - 5 binary: anemia status, diabetes status, high blood pressure status, sex, and smoking status
    - 7 numerical: age, creatine phosphokinase level, ejection fraction, platelet concentration, serum creatine level, serum sodium level, and length of follow-up period

# Motivation (Neha)

- Investigating heart failure, a condition impacting millions worldwide, and exploring its complex causes and contributing factors.

- Spotting patterns between risk factors and how heart failure progresses to get a better understanding.


# Linearity and Normality (Neha)

Normality

- Objective: Assess whether continuous predictor variables are normally distributed.

- Variables analyzed:
Age, Creatinine Phosphokinase, Ejection Fraction, Platelets, Serum Creatinine, Serum Sodium.


- Testing residuals ensures the model fits the data well and detects patterns that might indicate a need for model improvement.

- Tests performed: 

Shapiro-Wilk Test:
p-values < 0.05 for all variables â†’ None follow a normal distribution.

# Graphs
```{r, echo=FALSE, warning=FALSE, message=FALSE}
data <- read.csv("heart_failure_clinical_records_dataset.csv")

# Select a few continuous variables
selected_vars <- c("age", "serum_creatinine")

# Set up the plotting area to display two histograms and two Q-Q plots on one slide
par(mfrow = c(2, 2)) # 2 rows and 2 columns

# Loop through the selected variables to create histograms and Q-Q plots
for (var in selected_vars) {
  
  # Histogram for the selected variable
  hist(data[[var]], main = paste("Histogram of", var), 
       xlab = var, col = "skyblue", border = "white")
  
  # Q-Q Plot for the selected variable
  qqnorm(data[[var]], main = paste("Q-Q Plot of", var))
  qqline(data[[var]], col = "red")
}


```


# Linearity 

- Scatterplots of each variable vs. DEATH_EVENT:

- Fitted linear trendlines show non linear relationships for most variables.

- Logistic regression results:
Predictor variables (e.g., Age, Serum Creatinine) significantly linked to DEATH_EVENT likelihood.
Residual deviance was low, suggesting a good fit.


# Graphs 
```{r, echo=FALSE, warning=FALSE, message=FALSE}
model <- glm(DEATH_EVENT ~ age + serum_creatinine + ejection_fraction, data=data, family=binomial)

fitted_values <- fitted(model)
residuals <- residuals(model, type="pearson")
plot(fitted_values, residuals, main="Residuals vs Fitted Values", xlab="Fitted Values", ylab="Residuals")
abline(h=0, col="red")

```


# SVM & Random Forest (Paige)

# Logistic Regression (Witlie)

Logistic regression was an obvious first choice in tackling this binary classification problem. I created 3 models to assess predictive performance with different regularization methods.

- Ridge Regression
- Lasso
- No regularization

# Ridge Regression

```{r, echo=FALSE, warning=FALSE, message=FALSE}
set.seed(445)

library(tidymodels)
library(tidyverse)
library(ggplot2)
library(dplyr)

data <- read.csv("heart_failure_clinical_records_dataset.csv")

# scale continuous variables
numeric_vars <- sapply(data, is.numeric)
boolean_vars <- sapply(data, function(x) all(x %in% c(0,1)))

need_scale <- numeric_vars & !boolean_vars

data[need_scale] <- scale(data[need_scale])

# convert DEATH_EVENT to a factor 
data <- data %>%
  mutate(DEATH_EVENT = factor(DEATH_EVENT, levels = c("0", "1")))

data_split <- initial_split(data, prop = 0.8)
train_data <- training(data_split)
test_data <- testing(data_split)

prep_data <- recipe(DEATH_EVENT ~ ., data = train_data)

# create vector of 100 lambda values from 0.01 to 10^10
lambda <- lambda <- 10^seq(-2, 10, length.out = 100)
tune_df <- data.frame(lambda = lambda)

# fit ridge regression model for each lambda
prep_data <- recipe(DEATH_EVENT ~ ., data = data)

ridge_ests <- data.frame()

for(lam in lambda) {
  ridge_spec <- logistic_reg(mixture = 0, penalty = lam) |>
    set_mode("classification") |>
    set_engine("glmnet")
  
    workflow() |>
    add_model(ridge_spec) |>
    add_recipe(prep_data) |>
    fit(data) |>
    tidy() |>
    bind_rows(ridge_ests) -> ridge_ests
}

# visualize estimated coefficients for each lambda
ridge_ests |>
  filter(term != "(Intercept)") |>
  ggplot() +
  labs(title = "Ridge Regression: Coefficient Estimates vs Penalty") +
  geom_line(aes(penalty, estimate, group = term, colour = term)) +
  coord_trans(x = "log10")
```

# Ridge Regression

Using 10-fold cross-validation, I found the lambda with the highest ROC-AUC value of 0.887 was lambda = 0.0933

```{r, echo=FALSE, warning=FALSE, message=FALSE}
# perform 10-fold cross validation, estimate roc_auc for each lambda
data_10foldcv <- vfold_cv(data, v = 10)

ridge_spec <- logistic_reg(mixture = 0, penalty = tune("lambda")) |>
  set_mode("classification") |>
  set_engine("glmnet")

workflow() |>
  add_model(ridge_spec) |>
  add_recipe(prep_data) |>
  tune_grid(resamples = data_10foldcv, grid = tune_df, metrics = metric_set(roc_auc, accuracy)) -> ridge_tune

# determine best lambda

ridge_metrics <- ridge_tune |>
  collect_metrics() |>
  filter(.metric == "roc_auc") |>
  mutate(method = "Ridge")

ridge_best <- show_best(ridge_tune, metric = "roc_auc", n = 1)

ridge_tune |>
  collect_metrics() |>
  dplyr::select(lambda, .metric, mean) |>
  pivot_wider(names_from = .metric, values_from = mean) |>
  ggplot() +
  labs(title = "Ridge Regression: roc_auc vs lambda") +
  geom_line(aes(lambda, roc_auc)) + 
  geom_point(aes(lambda, roc_auc)) +
  geom_point(aes(ridge_best$lambda, ridge_best$mean), color = "red", shape = 1, size = 4) +
  coord_trans(x = "log10")
```

# Lasso

Lasso shows a steeper dropoff of coefficient estimates as a result of feature reduction

```{r, echo=FALSE, warning=FALSE, message=FALSE}
# fit lasso model for each lambda
lasso_ests <- data.frame()
for(lam in lambda) {
  lasso_spec <- logistic_reg(mixture = 1, penalty = lam) |>
    set_mode("classification") |>
    set_engine("glmnet")
  
    workflow() |>
    add_model(lasso_spec) |>
    add_recipe(prep_data) |>
    fit(data) |>
    tidy() |>
    bind_rows(lasso_ests) -> lasso_ests
}

# visualize estimated coefficients for each lambda
lasso_ests |>
  filter(term != "(Intercept)") |>
  ggplot() +
  labs(title = "Lasso Method: Coefficient Estimates vs Penalty") +
  geom_line(aes(penalty, estimate, group = term, colour = term)) +
  coord_trans(x = "log10")
```

# Lasso

Using 10-fold cross validation, I found the lambda with the highest ROC-AUC value of 0.886 was lambda = 0.01

```{r, echo=FALSE, warning=FALSE, message=FALSE}
# perform 10-fold cross validation, estimate roc_auc for each lambda
lasso_spec <- logistic_reg(mixture = 1, penalty = tune("lambda")) |>
  set_mode("classification") |>
  set_engine("glmnet")

workflow() |>
  add_model(lasso_spec) |>
  add_recipe(prep_data) |>
  tune_grid(resamples = data_10foldcv, grid = tune_df, metrics = metric_set(roc_auc, accuracy)) -> lasso_tune

lasso_metrics <- lasso_tune |>
  collect_metrics() |>
  filter(.metric == "roc_auc") |>
  mutate(method = "Lasso")

lasso_best <- show_best(lasso_tune, metric = "roc_auc", n = 1)

# show best lambda with lasso
lasso_tune |>
  collect_metrics() |>
  dplyr::select(lambda, .metric, mean) |>
  pivot_wider(names_from = .metric, values_from = mean) |>
  ggplot() +
  geom_line(aes(lambda, roc_auc)) +
  labs(title = "Lasso Method: ROC-AUC vs Lambda") +
  geom_point(aes(lambda, roc_auc)) +
  geom_point(aes(lasso_best$lambda, lasso_best$mean), color = "red", shape = 1, size = 4) +
  coord_trans(x = "log10")
```

# Results

- No Regularization:

- Ridge Regression:

- Lasso:


# SVM

```{r, echo=FALSE, warning=FALSE, message=FALSE}
set.seed(42)
scaled_data <- data
library(e1071)             
split <- sample.split(scaled_data$DEATH_EVENT, SplitRatio = 0.8)
train_data <- subset(scaled_data, split == TRUE)
test_data <- subset(scaled_data, split == FALSE)

svm_model <- svm(DEATH_EVENT ~ ., data = train_data, kernel = "radial", cost = 1, scale = TRUE)


summary(svm_model)

predictions <- predict(svm_model, test_data)

conf_matrix <- table(Predicted = predictions, Actual = test_data$DEATH_EVENT)
print(conf_matrix)

accuracy <- sum(predictions == test_data$DEATH_EVENT) / length(predictions)
print(paste("Accuracy: ", round(accuracy, 3)))

```



```{r, echo=FALSE, warning=FALSE, message=FALSE}
svm_data <- scaled_data[, c("age", "serum_creatinine", "DEATH_EVENT")]

svm_model_2d <- svm(DEATH_EVENT ~ age + serum_creatinine, data = svm_data, kernel = "radial")

x_range <- seq(min(svm_data$age) - 1, max(svm_data$age) + 1, length.out = 100)
y_range <- seq(min(svm_data$serum_creatinine) - 1, max(svm_data$serum_creatinine) + 1, length.out = 100)
grid <- expand.grid(age = x_range, serum_creatinine = y_range)

predictions_grid <- predict(svm_model_2d, grid)

ggplot(svm_data, aes(x = age, y = serum_creatinine)) +
  geom_point(alpha = 0.7, size = 3) +
  geom_tile(data = grid, 
            aes(x = age, y = serum_creatinine, 
                fill = predictions_grid), 
            alpha = 0.3) +
  scale_fill_manual(values = c("red", "blue")) +
  scale_color_manual(values = c("red", "blue")) +
  theme_minimal() +
  labs(title = "SVM Decision Boundary", x = "Age", y = "Serum Creatinine") +
  theme(legend.position = "none")

```

- The SVM Boundry analyzes the age vs serum creatinine using the death effect for males vs females. We can see that overall serium creatinine has little effect on the death effect especially in males. Meanwhile age and serum creatinine has a large response on females.


```{r, echo=FALSE, warning=FALSE, message=FALSE}
conf_matrix <- table(Predicted = predictions, Actual = test_data$DEATH_EVENT)

conf_matrix_df <- as.data.frame(conf_matrix)

ggplot(conf_matrix_df, aes(x = Predicted, y = Actual, fill = Freq)) + 
  geom_tile() +
  geom_text(aes(label = Freq), color = "white", size = 8) +
  scale_fill_gradient(low = "white", high = "blue") +
  labs(title = "Confusion Matrix", x = "Predicted", y = "Actual") +
  theme_minimal() +
  theme(axis.text.x = element_text(size = 12), axis.text.y = element_text(size = 12))

```

- Confusion Matrix is used to quantify our model's predictions, we can see the majority of true labels correctly matched with the predicted labels, indicating a high degree of accuracy.


```{r, echo=FALSE, warning=FALSE, message=FALSE}
svm_probs <- attr(predict(svm_model, test_data, decision.values = TRUE), "decision.values")

roc_curve <- roc(test_data$DEATH_EVENT, svm_probs)
plot(roc_curve, col = "blue", main = "ROC Curve for SVM Model")

```

- The SVM model, tuned with `C = 0.5` and `sigma = 0.0562`, had an ROC of 0.86, a sensitivity of 86%, and a specificity of 66%.


# Random Forest (paige)

```{r, echo=FALSE, warning=FALSE, message=FALSE}
rf_predictions <- predict(rf_model, test_data)

conf_matrix <- table(Predicted = rf_predictions, Actual = test_data$DEATH_EVENT)
print(conf_matrix)

accuracy <- sum(rf_predictions == test_data$DEATH_EVENT) / length(rf_predictions)
cat("Accuracy: ", round(accuracy, 3))
```


```{r, echo=FALSE, warning=FALSE, message=FALSE}

rf_probs <- predict(rf_model, test_data, type = "prob")[, 2] 
roc_curve <- roc(test_data$DEATH_EVENT, rf_probs)

plot(roc_curve, col = "blue", main = "ROC Curve for Random Forest Model")

```



- The Random Forest model performed best with `mtry = 2`, giving an ROC of 0.91. It was effective at identifying true positives (91% sensitivity) but had a moderate specificity of 69%.

- Overall, the Random Forest model performed better, especially in terms of overall accuracy and detecting positive cases.


# Results (Paige)

- None of the three logistic regression models performed well. The model without regularization performed very similarly to those which implemented ridge regression and lasso, indicating that regularization is unnecessary for this data.
- The SVM model provided more insight in making predictions however still wasn't the best option. 
- The Random Forest provides a sound approach to prediction as they use multiple trees and reduce risk of overfitting. 


# References 

Davide Chicco, Giuseppe Jurman: Machine learning can predict survival of patients with heart failure from serum creatinine and ejection fraction alone. BMC Medical Informatics and Decision Making 20, 16 (2020). https://bmcmedinformdecismak.biomedcentral.com/articles/10.1186/s12911-020-1023-5 
